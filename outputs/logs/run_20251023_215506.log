2025-10-23 21:55:06,926 | INFO | Logging initialized. Log file: outputs\logs\run_20251023_215506.log
2025-10-23 21:55:06,926 | INFO | Configuration file: config.yaml
2025-10-23 21:55:06,926 | INFO | Reward families to process: homogeneous
2025-10-23 21:55:06,926 | INFO | MA-SPI iterations=5, episodes/stage=10, episode_length=100
2025-10-23 21:55:06,926 | INFO | MA-LfL reward epochs=20000, reward batch size=8192
2025-10-23 21:55:06,927 | INFO | Log file located at outputs\logs\run_20251023_215506.log
2025-10-23 21:55:06,927 | INFO | Running experiment for reward family: homogeneous
2025-10-23 21:55:06,927 | INFO | [Experiment:homogeneous] Starting MA-SPI...
2025-10-23 21:55:07,733 | INFO | [MA-SPI] Starting run with 5 stages, 10 episodes/stage, episode length 100
2025-10-23 21:55:07,743 | INFO | [MA-SPI] Stage 1/5 - collecting trajectories...
2025-10-23 21:55:08,193 | INFO | [MA-SPI] Stage 1/5 - collected 2/10 episodes
2025-10-23 21:55:08,443 | INFO | [MA-SPI] Stage 1/5 - collected 4/10 episodes
2025-10-23 21:55:08,715 | INFO | [MA-SPI] Stage 1/5 - collected 6/10 episodes
2025-10-23 21:55:08,980 | INFO | [MA-SPI] Stage 1/5 - collected 8/10 episodes
2025-10-23 21:55:09,246 | INFO | [MA-SPI] Stage 1/5 - collected 10/10 episodes
2025-10-23 21:55:09,247 | INFO | [MA-SPI] Stage 1/5 - Agent 0 avg episode reward: -430.400 (std 25.703)
2025-10-23 21:55:09,247 | INFO | [MA-SPI] Stage 1/5 - Agent 1 avg episode reward: -407.900 (std 21.333)
2025-10-23 21:55:09,247 | INFO | [MA-SPI] Stage 1/5 - dataset size: 1000 transitions
2025-10-23 21:55:09,472 | INFO | [MA-SPI] Stage 1/5 - Q updates 20/100 (current batch 1000) [A0_Q_loss:0.6937, A1_Q_loss:0.6853]
2025-10-23 21:55:09,552 | INFO | [MA-SPI] Stage 1/5 - Q updates 40/100 (current batch 1000) [A0_Q_loss:0.3960, A1_Q_loss:0.5827]
2025-10-23 21:55:09,640 | INFO | [MA-SPI] Stage 1/5 - Q updates 60/100 (current batch 1000) [A0_Q_loss:0.3703, A1_Q_loss:0.5750]
2025-10-23 21:55:09,717 | INFO | [MA-SPI] Stage 1/5 - Q updates 80/100 (current batch 1000) [A0_Q_loss:0.3706, A1_Q_loss:0.5785]
2025-10-23 21:55:09,798 | INFO | [MA-SPI] Stage 1/5 - Q updates 100/100 (current batch 1000) [A0_Q_loss:0.3707, A1_Q_loss:0.5800]
2025-10-23 21:55:09,832 | INFO | [MA-SPI] Stage 1/5 - policy updates 10/50 (current batch 1000) [A0_Policy_loss:0.0170, A1_Policy_loss:0.0111]
2025-10-23 21:55:09,865 | INFO | [MA-SPI] Stage 1/5 - policy updates 20/50 (current batch 1000) [A0_Policy_loss:0.0044, A1_Policy_loss:0.0028]
2025-10-23 21:55:09,897 | INFO | [MA-SPI] Stage 1/5 - policy updates 30/50 (current batch 1000) [A0_Policy_loss:0.0018, A1_Policy_loss:0.0009]
2025-10-23 21:55:09,930 | INFO | [MA-SPI] Stage 1/5 - policy updates 40/50 (current batch 1000) [A0_Policy_loss:0.0008, A1_Policy_loss:0.0003]
2025-10-23 21:55:09,963 | INFO | [MA-SPI] Stage 1/5 - policy updates 50/50 (current batch 1000) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0001]
2025-10-23 21:55:09,964 | INFO | [MA-SPI] Stage 1/5 - completed
2025-10-23 21:55:09,964 | INFO | [MA-SPI] Stage 2/5 - collecting trajectories...
2025-10-23 21:55:10,223 | INFO | [MA-SPI] Stage 2/5 - collected 2/10 episodes
2025-10-23 21:55:10,466 | INFO | [MA-SPI] Stage 2/5 - collected 4/10 episodes
2025-10-23 21:55:10,714 | INFO | [MA-SPI] Stage 2/5 - collected 6/10 episodes
2025-10-23 21:55:10,971 | INFO | [MA-SPI] Stage 2/5 - collected 8/10 episodes
2025-10-23 21:55:11,212 | INFO | [MA-SPI] Stage 2/5 - collected 10/10 episodes
2025-10-23 21:55:11,213 | INFO | [MA-SPI] Stage 2/5 - Agent 0 avg episode reward: -332.800 (std 25.443)
2025-10-23 21:55:11,213 | INFO | [MA-SPI] Stage 2/5 - Agent 1 avg episode reward: -332.500 (std 30.270)
2025-10-23 21:55:11,214 | INFO | [MA-SPI] Stage 2/5 - dataset size: 1000 transitions
2025-10-23 21:55:11,292 | INFO | [MA-SPI] Stage 2/5 - Q updates 20/100 (current batch 1000) [A0_Q_loss:0.8750, A1_Q_loss:0.5975]
2025-10-23 21:55:11,370 | INFO | [MA-SPI] Stage 2/5 - Q updates 40/100 (current batch 1000) [A0_Q_loss:0.7798, A1_Q_loss:0.5613]
2025-10-23 21:55:11,448 | INFO | [MA-SPI] Stage 2/5 - Q updates 60/100 (current batch 1000) [A0_Q_loss:0.7016, A1_Q_loss:0.5019]
2025-10-23 21:55:11,529 | INFO | [MA-SPI] Stage 2/5 - Q updates 80/100 (current batch 1000) [A0_Q_loss:0.7002, A1_Q_loss:0.4854]
2025-10-23 21:55:11,611 | INFO | [MA-SPI] Stage 2/5 - Q updates 100/100 (current batch 1000) [A0_Q_loss:0.7080, A1_Q_loss:0.4915]
2025-10-23 21:55:11,642 | INFO | [MA-SPI] Stage 2/5 - policy updates 10/50 (current batch 1000) [A0_Policy_loss:0.0159, A1_Policy_loss:0.0131]
2025-10-23 21:55:11,679 | INFO | [MA-SPI] Stage 2/5 - policy updates 20/50 (current batch 1000) [A0_Policy_loss:0.0042, A1_Policy_loss:0.0040]
2025-10-23 21:55:11,713 | INFO | [MA-SPI] Stage 2/5 - policy updates 30/50 (current batch 1000) [A0_Policy_loss:0.0012, A1_Policy_loss:0.0014]
2025-10-23 21:55:11,745 | INFO | [MA-SPI] Stage 2/5 - policy updates 40/50 (current batch 1000) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0005]
2025-10-23 21:55:11,784 | INFO | [MA-SPI] Stage 2/5 - policy updates 50/50 (current batch 1000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0002]
2025-10-23 21:55:11,786 | INFO | [MA-SPI] Stage 2/5 - completed
2025-10-23 21:55:11,786 | INFO | [MA-SPI] Stage 3/5 - collecting trajectories...
2025-10-23 21:55:12,048 | INFO | [MA-SPI] Stage 3/5 - collected 2/10 episodes
2025-10-23 21:55:12,297 | INFO | [MA-SPI] Stage 3/5 - collected 4/10 episodes
2025-10-23 21:55:12,579 | INFO | [MA-SPI] Stage 3/5 - collected 6/10 episodes
2025-10-23 21:55:12,856 | INFO | [MA-SPI] Stage 3/5 - collected 8/10 episodes
2025-10-23 21:55:13,473 | INFO | [MA-SPI] Stage 3/5 - collected 10/10 episodes
2025-10-23 21:55:13,476 | INFO | [MA-SPI] Stage 3/5 - Agent 0 avg episode reward: -295.800 (std 18.867)
2025-10-23 21:55:13,476 | INFO | [MA-SPI] Stage 3/5 - Agent 1 avg episode reward: -300.200 (std 19.752)
2025-10-23 21:55:13,476 | INFO | [MA-SPI] Stage 3/5 - dataset size: 1000 transitions
2025-10-23 21:55:13,653 | INFO | [MA-SPI] Stage 3/5 - Q updates 20/100 (current batch 1000) [A0_Q_loss:0.4541, A1_Q_loss:0.6050]
2025-10-23 21:55:13,839 | INFO | [MA-SPI] Stage 3/5 - Q updates 40/100 (current batch 1000) [A0_Q_loss:0.4142, A1_Q_loss:0.4676]
2025-10-23 21:55:14,061 | INFO | [MA-SPI] Stage 3/5 - Q updates 60/100 (current batch 1000) [A0_Q_loss:0.4158, A1_Q_loss:0.4564]
2025-10-23 21:55:14,249 | INFO | [MA-SPI] Stage 3/5 - Q updates 80/100 (current batch 1000) [A0_Q_loss:0.3974, A1_Q_loss:0.4654]
2025-10-23 21:55:14,451 | INFO | [MA-SPI] Stage 3/5 - Q updates 100/100 (current batch 1000) [A0_Q_loss:0.3958, A1_Q_loss:0.4638]
2025-10-23 21:55:14,536 | INFO | [MA-SPI] Stage 3/5 - policy updates 10/50 (current batch 1000) [A0_Policy_loss:0.0131, A1_Policy_loss:0.0074]
2025-10-23 21:55:14,620 | INFO | [MA-SPI] Stage 3/5 - policy updates 20/50 (current batch 1000) [A0_Policy_loss:0.0039, A1_Policy_loss:0.0028]
2025-10-23 21:55:14,687 | INFO | [MA-SPI] Stage 3/5 - policy updates 30/50 (current batch 1000) [A0_Policy_loss:0.0012, A1_Policy_loss:0.0008]
2025-10-23 21:55:14,763 | INFO | [MA-SPI] Stage 3/5 - policy updates 40/50 (current batch 1000) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0002]
2025-10-23 21:55:14,838 | INFO | [MA-SPI] Stage 3/5 - policy updates 50/50 (current batch 1000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0000]
2025-10-23 21:55:14,839 | INFO | [MA-SPI] Stage 3/5 - completed
2025-10-23 21:55:14,839 | INFO | [MA-SPI] Stage 4/5 - collecting trajectories...
2025-10-23 21:55:15,386 | INFO | [MA-SPI] Stage 4/5 - collected 2/10 episodes
2025-10-23 21:55:15,941 | INFO | [MA-SPI] Stage 4/5 - collected 4/10 episodes
2025-10-23 21:55:16,527 | INFO | [MA-SPI] Stage 4/5 - collected 6/10 episodes
2025-10-23 21:55:17,054 | INFO | [MA-SPI] Stage 4/5 - collected 8/10 episodes
2025-10-23 21:55:17,653 | INFO | [MA-SPI] Stage 4/5 - collected 10/10 episodes
2025-10-23 21:55:17,655 | INFO | [MA-SPI] Stage 4/5 - Agent 0 avg episode reward: -265.800 (std 24.070)
2025-10-23 21:55:17,655 | INFO | [MA-SPI] Stage 4/5 - Agent 1 avg episode reward: -269.300 (std 17.641)
2025-10-23 21:55:17,656 | INFO | [MA-SPI] Stage 4/5 - dataset size: 1000 transitions
2025-10-23 21:55:17,828 | INFO | [MA-SPI] Stage 4/5 - Q updates 20/100 (current batch 1000) [A0_Q_loss:0.4093, A1_Q_loss:0.3209]
2025-10-23 21:55:18,036 | INFO | [MA-SPI] Stage 4/5 - Q updates 40/100 (current batch 1000) [A0_Q_loss:0.3501, A1_Q_loss:0.2943]
2025-10-23 21:55:18,214 | INFO | [MA-SPI] Stage 4/5 - Q updates 60/100 (current batch 1000) [A0_Q_loss:0.3378, A1_Q_loss:0.2696]
2025-10-23 21:55:18,411 | INFO | [MA-SPI] Stage 4/5 - Q updates 80/100 (current batch 1000) [A0_Q_loss:0.3391, A1_Q_loss:0.2686]
2025-10-23 21:55:18,622 | INFO | [MA-SPI] Stage 4/5 - Q updates 100/100 (current batch 1000) [A0_Q_loss:0.3349, A1_Q_loss:0.2692]
2025-10-23 21:55:18,701 | INFO | [MA-SPI] Stage 4/5 - policy updates 10/50 (current batch 1000) [A0_Policy_loss:0.0043, A1_Policy_loss:0.0047]
2025-10-23 21:55:18,786 | INFO | [MA-SPI] Stage 4/5 - policy updates 20/50 (current batch 1000) [A0_Policy_loss:0.0015, A1_Policy_loss:0.0015]
2025-10-23 21:55:18,875 | INFO | [MA-SPI] Stage 4/5 - policy updates 30/50 (current batch 1000) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0005]
2025-10-23 21:55:18,967 | INFO | [MA-SPI] Stage 4/5 - policy updates 40/50 (current batch 1000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0001]
2025-10-23 21:55:19,047 | INFO | [MA-SPI] Stage 4/5 - policy updates 50/50 (current batch 1000) [A0_Policy_loss:0.0001, A1_Policy_loss:-0.0000]
2025-10-23 21:55:19,048 | INFO | [MA-SPI] Stage 4/5 - completed
2025-10-23 21:55:19,048 | INFO | [MA-SPI] Stage 5/5 - collecting trajectories...
2025-10-23 21:55:19,586 | INFO | [MA-SPI] Stage 5/5 - collected 2/10 episodes
2025-10-23 21:55:20,134 | INFO | [MA-SPI] Stage 5/5 - collected 4/10 episodes
2025-10-23 21:55:20,653 | INFO | [MA-SPI] Stage 5/5 - collected 6/10 episodes
2025-10-23 21:55:21,232 | INFO | [MA-SPI] Stage 5/5 - collected 8/10 episodes
2025-10-23 21:55:21,778 | INFO | [MA-SPI] Stage 5/5 - collected 10/10 episodes
2025-10-23 21:55:21,782 | INFO | [MA-SPI] Stage 5/5 - Agent 0 avg episode reward: -267.500 (std 21.481)
2025-10-23 21:55:21,782 | INFO | [MA-SPI] Stage 5/5 - Agent 1 avg episode reward: -263.000 (std 22.869)
2025-10-23 21:55:21,783 | INFO | [MA-SPI] Stage 5/5 - dataset size: 1000 transitions
2025-10-23 21:55:21,954 | INFO | [MA-SPI] Stage 5/5 - Q updates 20/100 (current batch 1000) [A0_Q_loss:0.3165, A1_Q_loss:0.4460]
2025-10-23 21:55:22,123 | INFO | [MA-SPI] Stage 5/5 - Q updates 40/100 (current batch 1000) [A0_Q_loss:0.2773, A1_Q_loss:0.4132]
2025-10-23 21:55:22,307 | INFO | [MA-SPI] Stage 5/5 - Q updates 60/100 (current batch 1000) [A0_Q_loss:0.2850, A1_Q_loss:0.4086]
2025-10-23 21:55:22,488 | INFO | [MA-SPI] Stage 5/5 - Q updates 80/100 (current batch 1000) [A0_Q_loss:0.2740, A1_Q_loss:0.4054]
2025-10-23 21:55:22,682 | INFO | [MA-SPI] Stage 5/5 - Q updates 100/100 (current batch 1000) [A0_Q_loss:0.2773, A1_Q_loss:0.4002]
2025-10-23 21:55:22,803 | INFO | [MA-SPI] Stage 5/5 - policy updates 10/50 (current batch 1000) [A0_Policy_loss:0.0035, A1_Policy_loss:0.0032]
2025-10-23 21:55:22,891 | INFO | [MA-SPI] Stage 5/5 - policy updates 20/50 (current batch 1000) [A0_Policy_loss:0.0013, A1_Policy_loss:0.0011]
2025-10-23 21:55:22,971 | INFO | [MA-SPI] Stage 5/5 - policy updates 30/50 (current batch 1000) [A0_Policy_loss:0.0005, A1_Policy_loss:0.0004]
2025-10-23 21:55:23,045 | INFO | [MA-SPI] Stage 5/5 - policy updates 40/50 (current batch 1000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0001]
2025-10-23 21:55:23,114 | INFO | [MA-SPI] Stage 5/5 - policy updates 50/50 (current batch 1000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0000]
2025-10-23 21:55:23,114 | INFO | [MA-SPI] Stage 5/5 - completed
2025-10-23 21:55:23,184 | INFO | [Experiment:homogeneous] MA-SPI completed. Artifacts saved to outputs\homogeneous\ma_spi
2025-10-23 21:55:23,184 | INFO | [Experiment:homogeneous] Starting MA-LfL...
2025-10-23 21:55:23,185 | INFO | [MA-LfL] Estimating policies...
2025-10-23 21:55:23,185 | INFO | [MA-LfL][Policy] Stage 1/5 - preparing data
2025-10-23 21:55:23,250 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 0: 10/10 epochs completed (loss=1.0913, baseline=1.6094)
2025-10-23 21:55:23,284 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 1: 10/10 epochs completed (loss=1.0798, baseline=1.6094)
2025-10-23 21:55:23,285 | INFO | [MA-LfL][Policy] Stage 2/5 - preparing data
2025-10-23 21:55:23,328 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 0: 10/10 epochs completed (loss=1.0503, baseline=1.6094)
2025-10-23 21:55:23,367 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 1: 10/10 epochs completed (loss=1.0335, baseline=1.6094)
2025-10-23 21:55:23,368 | INFO | [MA-LfL][Policy] Stage 3/5 - preparing data
2025-10-23 21:55:23,407 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 0: 10/10 epochs completed (loss=1.0439, baseline=1.6094)
2025-10-23 21:55:23,446 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 1: 10/10 epochs completed (loss=1.0160, baseline=1.6094)
2025-10-23 21:55:23,447 | INFO | [MA-LfL][Policy] Stage 4/5 - preparing data
2025-10-23 21:55:23,486 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 0: 10/10 epochs completed (loss=0.9948, baseline=1.6094)
2025-10-23 21:55:23,526 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 1: 10/10 epochs completed (loss=1.0180, baseline=1.6094)
2025-10-23 21:55:23,527 | INFO | [MA-LfL][Policy] Stage 5/5 - preparing data
2025-10-23 21:55:23,570 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 0: 10/10 epochs completed (loss=1.0392, baseline=1.6094)
2025-10-23 21:55:23,607 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 1: 10/10 epochs completed (loss=0.9878, baseline=1.6094)
2025-10-23 21:55:23,608 | INFO | [MA-LfL] Computing reward targets...
2025-10-23 21:55:23,609 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 0
2025-10-23 21:55:24,278 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 1
2025-10-23 21:55:24,990 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 0
2025-10-23 21:55:25,763 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 1
2025-10-23 21:55:26,618 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 0
2025-10-23 21:55:27,397 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 1
2025-10-23 21:55:28,175 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 0
2025-10-23 21:55:28,976 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 1
2025-10-23 21:55:29,762 | INFO | [MA-LfL] Learning rewards...
2025-10-23 21:55:29,778 | INFO | [MA-LfL][Reward] Agent 0 - dataset size=4000 (batch=4000, epochs=20000)
2025-10-23 21:56:06,113 | INFO | [MA-LfL][Reward] Agent 0 - 2000/20000 epochs completed (latest loss=0.7961)
2025-10-23 21:56:33,444 | INFO | [MA-LfL][Reward] Agent 0 - 4000/20000 epochs completed (latest loss=0.7961)
2025-10-23 21:57:18,277 | INFO | [MA-LfL][Reward] Agent 0 - 6000/20000 epochs completed (latest loss=0.7961)
2025-10-23 21:58:03,753 | INFO | [MA-LfL][Reward] Agent 0 - 8000/20000 epochs completed (latest loss=0.7972)
2025-10-23 21:58:48,976 | INFO | [MA-LfL][Reward] Agent 0 - 10000/20000 epochs completed (latest loss=0.7962)
2025-10-23 21:59:37,136 | INFO | [MA-LfL][Reward] Agent 0 - 12000/20000 epochs completed (latest loss=0.7961)
2025-10-23 22:00:22,363 | INFO | [MA-LfL][Reward] Agent 0 - 14000/20000 epochs completed (latest loss=0.7964)
2025-10-23 22:00:45,185 | INFO | [MA-LfL][Reward] Agent 0 - 16000/20000 epochs completed (latest loss=0.7961)
2025-10-23 22:01:08,676 | INFO | [MA-LfL][Reward] Agent 0 - 18000/20000 epochs completed (latest loss=0.7961)
2025-10-23 22:01:30,297 | INFO | [MA-LfL][Reward] Agent 0 - 20000/20000 epochs completed (latest loss=0.7961)
2025-10-23 22:01:30,304 | INFO | [MA-LfL][Reward] Agent 1 - dataset size=4000 (batch=4000, epochs=20000)
2025-10-23 22:01:51,981 | INFO | [MA-LfL][Reward] Agent 1 - 2000/20000 epochs completed (latest loss=0.6948)
2025-10-23 22:02:13,835 | INFO | [MA-LfL][Reward] Agent 1 - 4000/20000 epochs completed (latest loss=0.6945)
2025-10-23 22:02:35,029 | INFO | [MA-LfL][Reward] Agent 1 - 6000/20000 epochs completed (latest loss=0.6943)
2025-10-23 22:02:55,889 | INFO | [MA-LfL][Reward] Agent 1 - 8000/20000 epochs completed (latest loss=0.6942)
2025-10-23 22:03:17,153 | INFO | [MA-LfL][Reward] Agent 1 - 10000/20000 epochs completed (latest loss=0.6940)
2025-10-23 22:03:39,334 | INFO | [MA-LfL][Reward] Agent 1 - 12000/20000 epochs completed (latest loss=0.6942)
2025-10-23 22:04:01,191 | INFO | [MA-LfL][Reward] Agent 1 - 14000/20000 epochs completed (latest loss=0.6936)
2025-10-23 22:04:27,825 | INFO | [MA-LfL][Reward] Agent 1 - 16000/20000 epochs completed (latest loss=0.6934)
2025-10-23 22:04:49,724 | INFO | [MA-LfL][Reward] Agent 1 - 18000/20000 epochs completed (latest loss=0.6933)
2025-10-23 22:05:34,999 | INFO | [MA-LfL][Reward] Agent 1 - 20000/20000 epochs completed (latest loss=0.6932)
2025-10-23 22:05:35,001 | INFO | [MA-LfL] Evaluating rewards...
2025-10-23 22:05:36,604 | INFO | [MA-LfL] Pipeline complete.
2025-10-23 22:05:36,604 | INFO | [Experiment:homogeneous] MA-LfL completed. Artifacts saved to outputs\homogeneous\ma_lfl
2025-10-23 22:05:36,604 | INFO | [Experiment:homogeneous] Agent 0 metrics - Pearson: 0.0952 | Spearman: 0.0608
2025-10-23 22:05:36,605 | INFO | [Experiment:homogeneous] Agent 1 metrics - Pearson: 0.1353 | Spearman: 0.1210
2025-10-23 22:05:36,605 | INFO | [Experiment:homogeneous] Trend stages: 1, 2, 3, 4
2025-10-23 22:05:36,605 | INFO | [Experiment:homogeneous] Trend Pearson: 0.2510, 0.3045, 0.4096, 0.3741
2025-10-23 22:05:36,605 | INFO | [Experiment:homogeneous] Trend Spearman: 0.1613, 0.2756, 0.3631, 0.3098
2025-10-23 22:05:36,607 | INFO | [Experiment:homogeneous] Agent 0 correlation >=0.4 ? NO
2025-10-23 22:05:36,607 | INFO | [Experiment:homogeneous] Agent 1 correlation >=0.4 ? NO
2025-10-23 22:05:36,607 | INFO | [Experiment:homogeneous] Correlation trend non-decreasing? NO
2025-10-23 22:05:36,609 | INFO | Summary saved to outputs\summary.json
2025-10-23 22:05:36,609 | INFO | Run complete.
