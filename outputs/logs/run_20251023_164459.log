2025-10-23 16:44:59,434 | INFO | Logging initialized. Log file: outputs\logs\run_20251023_164459.log
2025-10-23 16:44:59,434 | INFO | Configuration file: config.yaml
2025-10-23 16:44:59,435 | INFO | Reward families to process: heterogeneous
2025-10-23 16:44:59,435 | INFO | MA-SPI iterations=5, episodes/stage=20, episode_length=100
2025-10-23 16:44:59,435 | INFO | MA-LfL reward epochs=10000, reward batch size=8192
2025-10-23 16:44:59,435 | INFO | Log file located at outputs\logs\run_20251023_164459.log
2025-10-23 16:44:59,435 | INFO | Running experiment for reward family: heterogeneous
2025-10-23 16:44:59,435 | INFO | [Experiment:heterogeneous] Starting MA-SPI...
2025-10-23 16:45:00,291 | INFO | [MA-SPI] Starting run with 5 stages, 20 episodes/stage, episode length 100
2025-10-23 16:45:00,293 | INFO | [MA-SPI] Stage 1/5 - collecting trajectories...
2025-10-23 16:45:01,005 | INFO | [MA-SPI] Stage 1/5 - collected 4/20 episodes
2025-10-23 16:45:01,501 | INFO | [MA-SPI] Stage 1/5 - collected 8/20 episodes
2025-10-23 16:45:02,020 | INFO | [MA-SPI] Stage 1/5 - collected 12/20 episodes
2025-10-23 16:45:02,551 | INFO | [MA-SPI] Stage 1/5 - collected 16/20 episodes
2025-10-23 16:45:03,038 | INFO | [MA-SPI] Stage 1/5 - collected 20/20 episodes
2025-10-23 16:45:03,039 | INFO | [MA-SPI] Stage 1/5 - Agent 0 avg episode reward: -132.850 (std 30.775)
2025-10-23 16:45:03,039 | INFO | [MA-SPI] Stage 1/5 - Agent 1 avg episode reward: -116.650 (std 33.305)
2025-10-23 16:45:03,039 | INFO | [MA-SPI] Stage 1/5 - dataset size: 2000 transitions
2025-10-23 16:45:03,309 | INFO | [MA-SPI] Stage 1/5 - Q updates 20/100 (current batch 2000) [A0_Q_loss:2.9886, A1_Q_loss:3.1277]
2025-10-23 16:45:03,389 | INFO | [MA-SPI] Stage 1/5 - Q updates 40/100 (current batch 2000) [A0_Q_loss:1.3403, A1_Q_loss:1.4662]
2025-10-23 16:45:03,475 | INFO | [MA-SPI] Stage 1/5 - Q updates 60/100 (current batch 2000) [A0_Q_loss:0.8999, A1_Q_loss:0.9041]
2025-10-23 16:45:03,556 | INFO | [MA-SPI] Stage 1/5 - Q updates 80/100 (current batch 2000) [A0_Q_loss:0.6458, A1_Q_loss:0.6475]
2025-10-23 16:45:03,638 | INFO | [MA-SPI] Stage 1/5 - Q updates 100/100 (current batch 2000) [A0_Q_loss:0.5806, A1_Q_loss:0.5956]
2025-10-23 16:45:03,676 | INFO | [MA-SPI] Stage 1/5 - policy updates 10/50 (current batch 2000) [A0_Policy_loss:0.0267, A1_Policy_loss:0.0202]
2025-10-23 16:45:03,709 | INFO | [MA-SPI] Stage 1/5 - policy updates 20/50 (current batch 2000) [A0_Policy_loss:0.0088, A1_Policy_loss:0.0052]
2025-10-23 16:45:03,742 | INFO | [MA-SPI] Stage 1/5 - policy updates 30/50 (current batch 2000) [A0_Policy_loss:0.0029, A1_Policy_loss:0.0017]
2025-10-23 16:45:03,776 | INFO | [MA-SPI] Stage 1/5 - policy updates 40/50 (current batch 2000) [A0_Policy_loss:0.0009, A1_Policy_loss:0.0006]
2025-10-23 16:45:03,812 | INFO | [MA-SPI] Stage 1/5 - policy updates 50/50 (current batch 2000) [A0_Policy_loss:0.0003, A1_Policy_loss:0.0002]
2025-10-23 16:45:03,813 | INFO | [MA-SPI] Stage 1/5 - completed
2025-10-23 16:45:03,813 | INFO | [MA-SPI] Stage 2/5 - collecting trajectories...
2025-10-23 16:45:04,305 | INFO | [MA-SPI] Stage 2/5 - collected 4/20 episodes
2025-10-23 16:45:04,817 | INFO | [MA-SPI] Stage 2/5 - collected 8/20 episodes
2025-10-23 16:45:05,309 | INFO | [MA-SPI] Stage 2/5 - collected 12/20 episodes
2025-10-23 16:45:05,802 | INFO | [MA-SPI] Stage 2/5 - collected 16/20 episodes
2025-10-23 16:45:06,312 | INFO | [MA-SPI] Stage 2/5 - collected 20/20 episodes
2025-10-23 16:45:06,314 | INFO | [MA-SPI] Stage 2/5 - Agent 0 avg episode reward: 76.750 (std 49.654)
2025-10-23 16:45:06,314 | INFO | [MA-SPI] Stage 2/5 - Agent 1 avg episode reward: 80.200 (std 45.554)
2025-10-23 16:45:06,315 | INFO | [MA-SPI] Stage 2/5 - dataset size: 2000 transitions
2025-10-23 16:45:06,404 | INFO | [MA-SPI] Stage 2/5 - Q updates 20/100 (current batch 2000) [A0_Q_loss:1.6587, A1_Q_loss:1.6690]
2025-10-23 16:45:06,492 | INFO | [MA-SPI] Stage 2/5 - Q updates 40/100 (current batch 2000) [A0_Q_loss:0.8789, A1_Q_loss:1.1482]
2025-10-23 16:45:06,582 | INFO | [MA-SPI] Stage 2/5 - Q updates 60/100 (current batch 2000) [A0_Q_loss:0.7171, A1_Q_loss:1.0598]
2025-10-23 16:45:06,670 | INFO | [MA-SPI] Stage 2/5 - Q updates 80/100 (current batch 2000) [A0_Q_loss:0.6609, A1_Q_loss:1.0167]
2025-10-23 16:45:06,768 | INFO | [MA-SPI] Stage 2/5 - Q updates 100/100 (current batch 2000) [A0_Q_loss:0.6658, A1_Q_loss:1.0227]
2025-10-23 16:45:06,809 | INFO | [MA-SPI] Stage 2/5 - policy updates 10/50 (current batch 2000) [A0_Policy_loss:0.0107, A1_Policy_loss:0.0063]
2025-10-23 16:45:06,848 | INFO | [MA-SPI] Stage 2/5 - policy updates 20/50 (current batch 2000) [A0_Policy_loss:0.0035, A1_Policy_loss:0.0026]
2025-10-23 16:45:06,883 | INFO | [MA-SPI] Stage 2/5 - policy updates 30/50 (current batch 2000) [A0_Policy_loss:0.0012, A1_Policy_loss:0.0007]
2025-10-23 16:45:06,919 | INFO | [MA-SPI] Stage 2/5 - policy updates 40/50 (current batch 2000) [A0_Policy_loss:0.0006, A1_Policy_loss:0.0003]
2025-10-23 16:45:06,955 | INFO | [MA-SPI] Stage 2/5 - policy updates 50/50 (current batch 2000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0001]
2025-10-23 16:45:06,955 | INFO | [MA-SPI] Stage 2/5 - completed
2025-10-23 16:45:06,955 | INFO | [MA-SPI] Stage 3/5 - collecting trajectories...
2025-10-23 16:45:07,460 | INFO | [MA-SPI] Stage 3/5 - collected 4/20 episodes
2025-10-23 16:45:07,970 | INFO | [MA-SPI] Stage 3/5 - collected 8/20 episodes
2025-10-23 16:45:08,446 | INFO | [MA-SPI] Stage 3/5 - collected 12/20 episodes
2025-10-23 16:45:08,932 | INFO | [MA-SPI] Stage 3/5 - collected 16/20 episodes
2025-10-23 16:45:09,435 | INFO | [MA-SPI] Stage 3/5 - collected 20/20 episodes
2025-10-23 16:45:09,437 | INFO | [MA-SPI] Stage 3/5 - Agent 0 avg episode reward: 120.350 (std 22.141)
2025-10-23 16:45:09,437 | INFO | [MA-SPI] Stage 3/5 - Agent 1 avg episode reward: 118.850 (std 23.350)
2025-10-23 16:45:09,437 | INFO | [MA-SPI] Stage 3/5 - dataset size: 2000 transitions
2025-10-23 16:45:09,526 | INFO | [MA-SPI] Stage 3/5 - Q updates 20/100 (current batch 2000) [A0_Q_loss:0.4121, A1_Q_loss:0.4092]
2025-10-23 16:45:09,616 | INFO | [MA-SPI] Stage 3/5 - Q updates 40/100 (current batch 2000) [A0_Q_loss:0.3651, A1_Q_loss:0.3327]
2025-10-23 16:45:09,699 | INFO | [MA-SPI] Stage 3/5 - Q updates 60/100 (current batch 2000) [A0_Q_loss:0.3473, A1_Q_loss:0.3389]
2025-10-23 16:45:09,780 | INFO | [MA-SPI] Stage 3/5 - Q updates 80/100 (current batch 2000) [A0_Q_loss:0.3502, A1_Q_loss:0.3409]
2025-10-23 16:45:09,863 | INFO | [MA-SPI] Stage 3/5 - Q updates 100/100 (current batch 2000) [A0_Q_loss:0.3252, A1_Q_loss:0.3393]
2025-10-23 16:45:09,897 | INFO | [MA-SPI] Stage 3/5 - policy updates 10/50 (current batch 2000) [A0_Policy_loss:0.0023, A1_Policy_loss:0.0021]
2025-10-23 16:45:09,930 | INFO | [MA-SPI] Stage 3/5 - policy updates 20/50 (current batch 2000) [A0_Policy_loss:0.0007, A1_Policy_loss:0.0007]
2025-10-23 16:45:09,964 | INFO | [MA-SPI] Stage 3/5 - policy updates 30/50 (current batch 2000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0004]
2025-10-23 16:45:09,998 | INFO | [MA-SPI] Stage 3/5 - policy updates 40/50 (current batch 2000) [A0_Policy_loss:0.0000, A1_Policy_loss:0.0002]
2025-10-23 16:45:10,032 | INFO | [MA-SPI] Stage 3/5 - policy updates 50/50 (current batch 2000) [A0_Policy_loss:-0.0001, A1_Policy_loss:0.0001]
2025-10-23 16:45:10,032 | INFO | [MA-SPI] Stage 3/5 - completed
2025-10-23 16:45:10,032 | INFO | [MA-SPI] Stage 4/5 - collecting trajectories...
2025-10-23 16:45:10,549 | INFO | [MA-SPI] Stage 4/5 - collected 4/20 episodes
2025-10-23 16:45:11,044 | INFO | [MA-SPI] Stage 4/5 - collected 8/20 episodes
2025-10-23 16:45:11,526 | INFO | [MA-SPI] Stage 4/5 - collected 12/20 episodes
2025-10-23 16:45:12,015 | INFO | [MA-SPI] Stage 4/5 - collected 16/20 episodes
2025-10-23 16:45:12,505 | INFO | [MA-SPI] Stage 4/5 - collected 20/20 episodes
2025-10-23 16:45:12,507 | INFO | [MA-SPI] Stage 4/5 - Agent 0 avg episode reward: 110.850 (std 14.301)
2025-10-23 16:45:12,507 | INFO | [MA-SPI] Stage 4/5 - Agent 1 avg episode reward: 109.300 (std 18.155)
2025-10-23 16:45:12,507 | INFO | [MA-SPI] Stage 4/5 - dataset size: 2000 transitions
2025-10-23 16:45:12,595 | INFO | [MA-SPI] Stage 4/5 - Q updates 20/100 (current batch 2000) [A0_Q_loss:0.5446, A1_Q_loss:0.3711]
2025-10-23 16:45:12,681 | INFO | [MA-SPI] Stage 4/5 - Q updates 40/100 (current batch 2000) [A0_Q_loss:0.5745, A1_Q_loss:0.3539]
2025-10-23 16:45:12,769 | INFO | [MA-SPI] Stage 4/5 - Q updates 60/100 (current batch 2000) [A0_Q_loss:0.5527, A1_Q_loss:0.3205]
2025-10-23 16:45:12,851 | INFO | [MA-SPI] Stage 4/5 - Q updates 80/100 (current batch 2000) [A0_Q_loss:0.5507, A1_Q_loss:0.3122]
2025-10-23 16:45:12,932 | INFO | [MA-SPI] Stage 4/5 - Q updates 100/100 (current batch 2000) [A0_Q_loss:0.5485, A1_Q_loss:0.3345]
2025-10-23 16:45:12,966 | INFO | [MA-SPI] Stage 4/5 - policy updates 10/50 (current batch 2000) [A0_Policy_loss:0.0028, A1_Policy_loss:0.0019]
2025-10-23 16:45:13,001 | INFO | [MA-SPI] Stage 4/5 - policy updates 20/50 (current batch 2000) [A0_Policy_loss:0.0009, A1_Policy_loss:0.0007]
2025-10-23 16:45:13,039 | INFO | [MA-SPI] Stage 4/5 - policy updates 30/50 (current batch 2000) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0003]
2025-10-23 16:45:13,073 | INFO | [MA-SPI] Stage 4/5 - policy updates 40/50 (current batch 2000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0001]
2025-10-23 16:45:13,109 | INFO | [MA-SPI] Stage 4/5 - policy updates 50/50 (current batch 2000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0001]
2025-10-23 16:45:13,109 | INFO | [MA-SPI] Stage 4/5 - completed
2025-10-23 16:45:13,109 | INFO | [MA-SPI] Stage 5/5 - collecting trajectories...
2025-10-23 16:45:13,599 | INFO | [MA-SPI] Stage 5/5 - collected 4/20 episodes
2025-10-23 16:45:14,096 | INFO | [MA-SPI] Stage 5/5 - collected 8/20 episodes
2025-10-23 16:45:14,586 | INFO | [MA-SPI] Stage 5/5 - collected 12/20 episodes
2025-10-23 16:45:15,085 | INFO | [MA-SPI] Stage 5/5 - collected 16/20 episodes
2025-10-23 16:45:15,601 | INFO | [MA-SPI] Stage 5/5 - collected 20/20 episodes
2025-10-23 16:45:15,603 | INFO | [MA-SPI] Stage 5/5 - Agent 0 avg episode reward: 97.950 (std 29.371)
2025-10-23 16:45:15,603 | INFO | [MA-SPI] Stage 5/5 - Agent 1 avg episode reward: 100.900 (std 30.341)
2025-10-23 16:45:15,603 | INFO | [MA-SPI] Stage 5/5 - dataset size: 2000 transitions
2025-10-23 16:45:15,692 | INFO | [MA-SPI] Stage 5/5 - Q updates 20/100 (current batch 2000) [A0_Q_loss:0.5222, A1_Q_loss:0.8815]
2025-10-23 16:45:15,778 | INFO | [MA-SPI] Stage 5/5 - Q updates 40/100 (current batch 2000) [A0_Q_loss:0.4161, A1_Q_loss:0.7347]
2025-10-23 16:45:15,861 | INFO | [MA-SPI] Stage 5/5 - Q updates 60/100 (current batch 2000) [A0_Q_loss:0.4195, A1_Q_loss:0.6797]
2025-10-23 16:45:15,954 | INFO | [MA-SPI] Stage 5/5 - Q updates 80/100 (current batch 2000) [A0_Q_loss:0.3962, A1_Q_loss:0.6997]
2025-10-23 16:45:16,045 | INFO | [MA-SPI] Stage 5/5 - Q updates 100/100 (current batch 2000) [A0_Q_loss:0.3944, A1_Q_loss:0.6768]
2025-10-23 16:45:16,078 | INFO | [MA-SPI] Stage 5/5 - policy updates 10/50 (current batch 2000) [A0_Policy_loss:0.0057, A1_Policy_loss:0.0053]
2025-10-23 16:45:16,115 | INFO | [MA-SPI] Stage 5/5 - policy updates 20/50 (current batch 2000) [A0_Policy_loss:0.0017, A1_Policy_loss:0.0019]
2025-10-23 16:45:16,151 | INFO | [MA-SPI] Stage 5/5 - policy updates 30/50 (current batch 2000) [A0_Policy_loss:0.0006, A1_Policy_loss:0.0006]
2025-10-23 16:45:16,186 | INFO | [MA-SPI] Stage 5/5 - policy updates 40/50 (current batch 2000) [A0_Policy_loss:0.0003, A1_Policy_loss:0.0002]
2025-10-23 16:45:16,221 | INFO | [MA-SPI] Stage 5/5 - policy updates 50/50 (current batch 2000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0001]
2025-10-23 16:45:16,221 | INFO | [MA-SPI] Stage 5/5 - completed
2025-10-23 16:45:16,272 | INFO | [Experiment:heterogeneous] MA-SPI completed. Artifacts saved to outputs\heterogeneous\ma_spi
2025-10-23 16:45:16,272 | INFO | [Experiment:heterogeneous] Starting MA-LfL...
2025-10-23 16:45:16,273 | INFO | [MA-LfL] Estimating policies...
2025-10-23 16:45:16,273 | INFO | [MA-LfL][Policy] Stage 1/5 - preparing data
2025-10-23 16:45:16,325 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 0: 10/10 epochs completed (loss=1.1243, baseline=1.6094)
2025-10-23 16:45:16,345 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 1: 10/10 epochs completed (loss=1.1204, baseline=1.6094)
2025-10-23 16:45:16,346 | INFO | [MA-LfL][Policy] Stage 2/5 - preparing data
2025-10-23 16:45:16,370 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 0: 10/10 epochs completed (loss=1.0694, baseline=1.6094)
2025-10-23 16:45:16,393 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 1: 10/10 epochs completed (loss=1.0455, baseline=1.6094)
2025-10-23 16:45:16,394 | INFO | [MA-LfL][Policy] Stage 3/5 - preparing data
2025-10-23 16:45:16,421 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 0: 10/10 epochs completed (loss=1.0166, baseline=1.6094)
2025-10-23 16:45:16,444 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 1: 10/10 epochs completed (loss=1.0013, baseline=1.6094)
2025-10-23 16:45:16,444 | INFO | [MA-LfL][Policy] Stage 4/5 - preparing data
2025-10-23 16:45:16,462 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 0: 10/10 epochs completed (loss=1.0123, baseline=1.6094)
2025-10-23 16:45:16,483 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 1: 10/10 epochs completed (loss=1.0500, baseline=1.6094)
2025-10-23 16:45:16,483 | INFO | [MA-LfL][Policy] Stage 5/5 - preparing data
2025-10-23 16:45:16,503 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 0: 10/10 epochs completed (loss=1.0388, baseline=1.6094)
2025-10-23 16:45:16,522 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 1: 10/10 epochs completed (loss=1.0230, baseline=1.6094)
2025-10-23 16:45:16,522 | INFO | [MA-LfL] Computing reward targets...
2025-10-23 16:45:16,523 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 0
2025-10-23 16:45:16,845 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 1
2025-10-23 16:45:17,159 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 0
2025-10-23 16:45:17,468 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 1
2025-10-23 16:45:17,802 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 0
2025-10-23 16:45:18,171 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 1
2025-10-23 16:45:18,545 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 0
2025-10-23 16:45:18,964 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 1
2025-10-23 16:45:19,348 | INFO | [MA-LfL] Learning rewards...
2025-10-23 16:45:19,359 | INFO | [MA-LfL][Reward] Agent 0 - dataset size=8000 (batch=8000, epochs=10000)
2025-10-23 16:45:29,877 | INFO | [MA-LfL][Reward] Agent 0 - 1000/10000 epochs completed (latest loss=0.6753)
2025-10-23 16:45:40,332 | INFO | [MA-LfL][Reward] Agent 0 - 2000/10000 epochs completed (latest loss=0.6758)
2025-10-23 16:45:50,929 | INFO | [MA-LfL][Reward] Agent 0 - 3000/10000 epochs completed (latest loss=0.6751)
2025-10-23 16:46:01,451 | INFO | [MA-LfL][Reward] Agent 0 - 4000/10000 epochs completed (latest loss=0.6751)
2025-10-23 16:46:12,338 | INFO | [MA-LfL][Reward] Agent 0 - 5000/10000 epochs completed (latest loss=0.6753)
2025-10-23 16:46:22,724 | INFO | [MA-LfL][Reward] Agent 0 - 6000/10000 epochs completed (latest loss=0.6751)
2025-10-23 16:46:32,704 | INFO | [MA-LfL][Reward] Agent 0 - 7000/10000 epochs completed (latest loss=0.6754)
2025-10-23 16:46:43,224 | INFO | [MA-LfL][Reward] Agent 0 - 8000/10000 epochs completed (latest loss=0.6751)
2025-10-23 16:46:53,777 | INFO | [MA-LfL][Reward] Agent 0 - 9000/10000 epochs completed (latest loss=0.6751)
2025-10-23 16:47:04,459 | INFO | [MA-LfL][Reward] Agent 0 - 10000/10000 epochs completed (latest loss=0.6752)
2025-10-23 16:47:04,467 | INFO | [MA-LfL][Reward] Agent 1 - dataset size=8000 (batch=8000, epochs=10000)
2025-10-23 16:47:15,407 | INFO | [MA-LfL][Reward] Agent 1 - 1000/10000 epochs completed (latest loss=0.5158)
2025-10-23 16:47:26,092 | INFO | [MA-LfL][Reward] Agent 1 - 2000/10000 epochs completed (latest loss=0.5157)
2025-10-23 16:47:36,650 | INFO | [MA-LfL][Reward] Agent 1 - 3000/10000 epochs completed (latest loss=0.5170)
2025-10-23 16:47:46,978 | INFO | [MA-LfL][Reward] Agent 1 - 4000/10000 epochs completed (latest loss=0.5153)
2025-10-23 16:47:56,969 | INFO | [MA-LfL][Reward] Agent 1 - 5000/10000 epochs completed (latest loss=0.5153)
2025-10-23 16:48:06,994 | INFO | [MA-LfL][Reward] Agent 1 - 6000/10000 epochs completed (latest loss=0.5155)
2025-10-23 16:48:17,054 | INFO | [MA-LfL][Reward] Agent 1 - 7000/10000 epochs completed (latest loss=0.5153)
2025-10-23 16:48:27,266 | INFO | [MA-LfL][Reward] Agent 1 - 8000/10000 epochs completed (latest loss=0.5154)
2025-10-23 16:48:38,211 | INFO | [MA-LfL][Reward] Agent 1 - 9000/10000 epochs completed (latest loss=0.5188)
2025-10-23 16:48:49,200 | INFO | [MA-LfL][Reward] Agent 1 - 10000/10000 epochs completed (latest loss=0.5299)
2025-10-23 16:48:49,200 | INFO | [MA-LfL] Evaluating rewards...
2025-10-23 16:48:49,958 | INFO | [MA-LfL] Pipeline complete.
2025-10-23 16:48:49,959 | INFO | [Experiment:heterogeneous] MA-LfL completed. Artifacts saved to outputs\heterogeneous\ma_lfl
2025-10-23 16:48:49,959 | INFO | [Experiment:heterogeneous] Agent 0 metrics - Pearson: 0.1773 | Spearman: 0.1892
2025-10-23 16:48:49,959 | INFO | [Experiment:heterogeneous] Agent 1 metrics - Pearson: 0.1781 | Spearman: 0.2107
2025-10-23 16:48:49,959 | INFO | [Experiment:heterogeneous] Trend stages: 1, 2, 3, 4
2025-10-23 16:48:49,959 | INFO | [Experiment:heterogeneous] Trend Pearson: 0.4370, 0.5149, 0.5703, 0.5361
2025-10-23 16:48:49,959 | INFO | [Experiment:heterogeneous] Trend Spearman: 0.3960, 0.4687, 0.5539, 0.5350
2025-10-23 16:48:49,959 | INFO | [Experiment:heterogeneous] Agent 0 correlation >=0.4 ? NO
2025-10-23 16:48:49,959 | INFO | [Experiment:heterogeneous] Agent 1 correlation >=0.4 ? NO
2025-10-23 16:48:49,959 | INFO | [Experiment:heterogeneous] Correlation trend non-decreasing? NO
2025-10-23 16:48:49,961 | INFO | Summary saved to outputs\summary.json
2025-10-23 16:48:49,961 | INFO | Run complete.
