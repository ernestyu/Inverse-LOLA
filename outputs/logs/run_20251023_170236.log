2025-10-23 17:02:36,893 | INFO | Logging initialized. Log file: outputs\logs\run_20251023_170236.log
2025-10-23 17:02:36,893 | INFO | Configuration file: config.yaml
2025-10-23 17:02:36,893 | INFO | Reward families to process: heterogeneous
2025-10-23 17:02:36,894 | INFO | MA-SPI iterations=5, episodes/stage=5, episode_length=100
2025-10-23 17:02:36,894 | INFO | MA-LfL reward epochs=10000, reward batch size=8192
2025-10-23 17:02:36,894 | INFO | Log file located at outputs\logs\run_20251023_170236.log
2025-10-23 17:02:36,894 | INFO | Running experiment for reward family: heterogeneous
2025-10-23 17:02:36,894 | INFO | [Experiment:heterogeneous] Starting MA-SPI...
2025-10-23 17:02:37,739 | INFO | [MA-SPI] Starting run with 5 stages, 5 episodes/stage, episode length 100
2025-10-23 17:02:37,740 | INFO | [MA-SPI] Stage 1/5 - collecting trajectories...
2025-10-23 17:02:38,053 | INFO | [MA-SPI] Stage 1/5 - collected 1/5 episodes
2025-10-23 17:02:38,187 | INFO | [MA-SPI] Stage 1/5 - collected 2/5 episodes
2025-10-23 17:02:38,320 | INFO | [MA-SPI] Stage 1/5 - collected 3/5 episodes
2025-10-23 17:02:38,449 | INFO | [MA-SPI] Stage 1/5 - collected 4/5 episodes
2025-10-23 17:02:38,585 | INFO | [MA-SPI] Stage 1/5 - collected 5/5 episodes
2025-10-23 17:02:38,586 | INFO | [MA-SPI] Stage 1/5 - Agent 0 avg episode reward: -122.200 (std 34.429)
2025-10-23 17:02:38,586 | INFO | [MA-SPI] Stage 1/5 - Agent 1 avg episode reward: -92.800 (std 24.498)
2025-10-23 17:02:38,586 | INFO | [MA-SPI] Stage 1/5 - dataset size: 500 transitions
2025-10-23 17:02:38,831 | INFO | [MA-SPI] Stage 1/5 - Q updates 20/100 (current batch 500) [A0_Q_loss:3.0366, A1_Q_loss:3.1658]
2025-10-23 17:02:38,916 | INFO | [MA-SPI] Stage 1/5 - Q updates 40/100 (current batch 500) [A0_Q_loss:1.6741, A1_Q_loss:1.4715]
2025-10-23 17:02:38,999 | INFO | [MA-SPI] Stage 1/5 - Q updates 60/100 (current batch 500) [A0_Q_loss:1.0414, A1_Q_loss:0.9307]
2025-10-23 17:02:39,096 | INFO | [MA-SPI] Stage 1/5 - Q updates 80/100 (current batch 500) [A0_Q_loss:0.7208, A1_Q_loss:0.7172]
2025-10-23 17:02:39,181 | INFO | [MA-SPI] Stage 1/5 - Q updates 100/100 (current batch 500) [A0_Q_loss:0.5463, A1_Q_loss:0.6618]
2025-10-23 17:02:39,221 | INFO | [MA-SPI] Stage 1/5 - policy updates 10/50 (current batch 500) [A0_Policy_loss:0.0238, A1_Policy_loss:0.0133]
2025-10-23 17:02:39,259 | INFO | [MA-SPI] Stage 1/5 - policy updates 20/50 (current batch 500) [A0_Policy_loss:0.0061, A1_Policy_loss:0.0032]
2025-10-23 17:02:39,295 | INFO | [MA-SPI] Stage 1/5 - policy updates 30/50 (current batch 500) [A0_Policy_loss:0.0021, A1_Policy_loss:0.0010]
2025-10-23 17:02:39,333 | INFO | [MA-SPI] Stage 1/5 - policy updates 40/50 (current batch 500) [A0_Policy_loss:0.0005, A1_Policy_loss:0.0004]
2025-10-23 17:02:39,375 | INFO | [MA-SPI] Stage 1/5 - policy updates 50/50 (current batch 500) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0002]
2025-10-23 17:02:39,375 | INFO | [MA-SPI] Stage 1/5 - completed
2025-10-23 17:02:39,375 | INFO | [MA-SPI] Stage 2/5 - collecting trajectories...
2025-10-23 17:02:39,507 | INFO | [MA-SPI] Stage 2/5 - collected 1/5 episodes
2025-10-23 17:02:39,645 | INFO | [MA-SPI] Stage 2/5 - collected 2/5 episodes
2025-10-23 17:02:39,785 | INFO | [MA-SPI] Stage 2/5 - collected 3/5 episodes
2025-10-23 17:02:39,920 | INFO | [MA-SPI] Stage 2/5 - collected 4/5 episodes
2025-10-23 17:02:40,054 | INFO | [MA-SPI] Stage 2/5 - collected 5/5 episodes
2025-10-23 17:02:40,055 | INFO | [MA-SPI] Stage 2/5 - Agent 0 avg episode reward: -36.400 (std 23.661)
2025-10-23 17:02:40,055 | INFO | [MA-SPI] Stage 2/5 - Agent 1 avg episode reward: -12.800 (std 33.409)
2025-10-23 17:02:40,056 | INFO | [MA-SPI] Stage 2/5 - dataset size: 500 transitions
2025-10-23 17:02:40,140 | INFO | [MA-SPI] Stage 2/5 - Q updates 20/100 (current batch 500) [A0_Q_loss:3.3120, A1_Q_loss:1.4377]
2025-10-23 17:02:40,222 | INFO | [MA-SPI] Stage 2/5 - Q updates 40/100 (current batch 500) [A0_Q_loss:1.3934, A1_Q_loss:1.1925]
2025-10-23 17:02:40,302 | INFO | [MA-SPI] Stage 2/5 - Q updates 60/100 (current batch 500) [A0_Q_loss:0.8639, A1_Q_loss:1.1232]
2025-10-23 17:02:40,385 | INFO | [MA-SPI] Stage 2/5 - Q updates 80/100 (current batch 500) [A0_Q_loss:0.8164, A1_Q_loss:1.1111]
2025-10-23 17:02:40,466 | INFO | [MA-SPI] Stage 2/5 - Q updates 100/100 (current batch 500) [A0_Q_loss:0.7715, A1_Q_loss:1.0875]
2025-10-23 17:02:40,500 | INFO | [MA-SPI] Stage 2/5 - policy updates 10/50 (current batch 500) [A0_Policy_loss:0.0183, A1_Policy_loss:0.0221]
2025-10-23 17:02:40,541 | INFO | [MA-SPI] Stage 2/5 - policy updates 20/50 (current batch 500) [A0_Policy_loss:0.0071, A1_Policy_loss:0.0064]
2025-10-23 17:02:40,577 | INFO | [MA-SPI] Stage 2/5 - policy updates 30/50 (current batch 500) [A0_Policy_loss:0.0019, A1_Policy_loss:0.0019]
2025-10-23 17:02:40,612 | INFO | [MA-SPI] Stage 2/5 - policy updates 40/50 (current batch 500) [A0_Policy_loss:0.0008, A1_Policy_loss:0.0006]
2025-10-23 17:02:40,648 | INFO | [MA-SPI] Stage 2/5 - policy updates 50/50 (current batch 500) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0002]
2025-10-23 17:02:40,648 | INFO | [MA-SPI] Stage 2/5 - completed
2025-10-23 17:02:40,648 | INFO | [MA-SPI] Stage 3/5 - collecting trajectories...
2025-10-23 17:02:40,782 | INFO | [MA-SPI] Stage 3/5 - collected 1/5 episodes
2025-10-23 17:02:40,907 | INFO | [MA-SPI] Stage 3/5 - collected 2/5 episodes
2025-10-23 17:02:41,036 | INFO | [MA-SPI] Stage 3/5 - collected 3/5 episodes
2025-10-23 17:02:41,188 | INFO | [MA-SPI] Stage 3/5 - collected 4/5 episodes
2025-10-23 17:02:41,331 | INFO | [MA-SPI] Stage 3/5 - collected 5/5 episodes
2025-10-23 17:02:41,332 | INFO | [MA-SPI] Stage 3/5 - Agent 0 avg episode reward: 80.400 (std 8.499)
2025-10-23 17:02:41,332 | INFO | [MA-SPI] Stage 3/5 - Agent 1 avg episode reward: 91.600 (std 16.057)
2025-10-23 17:02:41,332 | INFO | [MA-SPI] Stage 3/5 - dataset size: 500 transitions
2025-10-23 17:02:41,424 | INFO | [MA-SPI] Stage 3/5 - Q updates 20/100 (current batch 500) [A0_Q_loss:0.8229, A1_Q_loss:1.5617]
2025-10-23 17:02:41,514 | INFO | [MA-SPI] Stage 3/5 - Q updates 40/100 (current batch 500) [A0_Q_loss:0.6274, A1_Q_loss:1.2978]
2025-10-23 17:02:41,599 | INFO | [MA-SPI] Stage 3/5 - Q updates 60/100 (current batch 500) [A0_Q_loss:0.5950, A1_Q_loss:1.0788]
2025-10-23 17:02:41,684 | INFO | [MA-SPI] Stage 3/5 - Q updates 80/100 (current batch 500) [A0_Q_loss:0.5770, A1_Q_loss:1.0693]
2025-10-23 17:02:41,771 | INFO | [MA-SPI] Stage 3/5 - Q updates 100/100 (current batch 500) [A0_Q_loss:0.5643, A1_Q_loss:1.0668]
2025-10-23 17:02:41,806 | INFO | [MA-SPI] Stage 3/5 - policy updates 10/50 (current batch 500) [A0_Policy_loss:0.0087, A1_Policy_loss:0.0189]
2025-10-23 17:02:41,846 | INFO | [MA-SPI] Stage 3/5 - policy updates 20/50 (current batch 500) [A0_Policy_loss:0.0034, A1_Policy_loss:0.0070]
2025-10-23 17:02:41,881 | INFO | [MA-SPI] Stage 3/5 - policy updates 30/50 (current batch 500) [A0_Policy_loss:0.0011, A1_Policy_loss:0.0023]
2025-10-23 17:02:41,915 | INFO | [MA-SPI] Stage 3/5 - policy updates 40/50 (current batch 500) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0010]
2025-10-23 17:02:41,953 | INFO | [MA-SPI] Stage 3/5 - policy updates 50/50 (current batch 500) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0004]
2025-10-23 17:02:41,953 | INFO | [MA-SPI] Stage 3/5 - completed
2025-10-23 17:02:41,954 | INFO | [MA-SPI] Stage 4/5 - collecting trajectories...
2025-10-23 17:02:42,081 | INFO | [MA-SPI] Stage 4/5 - collected 1/5 episodes
2025-10-23 17:02:42,203 | INFO | [MA-SPI] Stage 4/5 - collected 2/5 episodes
2025-10-23 17:02:42,331 | INFO | [MA-SPI] Stage 4/5 - collected 3/5 episodes
2025-10-23 17:02:42,459 | INFO | [MA-SPI] Stage 4/5 - collected 4/5 episodes
2025-10-23 17:02:42,592 | INFO | [MA-SPI] Stage 4/5 - collected 5/5 episodes
2025-10-23 17:02:42,592 | INFO | [MA-SPI] Stage 4/5 - Agent 0 avg episode reward: 111.000 (std 25.884)
2025-10-23 17:02:42,592 | INFO | [MA-SPI] Stage 4/5 - Agent 1 avg episode reward: 96.400 (std 24.287)
2025-10-23 17:02:42,593 | INFO | [MA-SPI] Stage 4/5 - dataset size: 500 transitions
2025-10-23 17:02:42,681 | INFO | [MA-SPI] Stage 4/5 - Q updates 20/100 (current batch 500) [A0_Q_loss:0.4050, A1_Q_loss:0.8372]
2025-10-23 17:02:42,768 | INFO | [MA-SPI] Stage 4/5 - Q updates 40/100 (current batch 500) [A0_Q_loss:0.2943, A1_Q_loss:0.7565]
2025-10-23 17:02:42,853 | INFO | [MA-SPI] Stage 4/5 - Q updates 60/100 (current batch 500) [A0_Q_loss:0.3137, A1_Q_loss:0.6285]
2025-10-23 17:02:42,935 | INFO | [MA-SPI] Stage 4/5 - Q updates 80/100 (current batch 500) [A0_Q_loss:0.2748, A1_Q_loss:0.6178]
2025-10-23 17:02:43,020 | INFO | [MA-SPI] Stage 4/5 - Q updates 100/100 (current batch 500) [A0_Q_loss:0.2514, A1_Q_loss:0.6126]
2025-10-23 17:02:43,053 | INFO | [MA-SPI] Stage 4/5 - policy updates 10/50 (current batch 500) [A0_Policy_loss:0.0046, A1_Policy_loss:0.0052]
2025-10-23 17:02:43,089 | INFO | [MA-SPI] Stage 4/5 - policy updates 20/50 (current batch 500) [A0_Policy_loss:0.0018, A1_Policy_loss:0.0020]
2025-10-23 17:02:43,125 | INFO | [MA-SPI] Stage 4/5 - policy updates 30/50 (current batch 500) [A0_Policy_loss:0.0007, A1_Policy_loss:0.0008]
2025-10-23 17:02:43,159 | INFO | [MA-SPI] Stage 4/5 - policy updates 40/50 (current batch 500) [A0_Policy_loss:0.0003, A1_Policy_loss:0.0003]
2025-10-23 17:02:43,193 | INFO | [MA-SPI] Stage 4/5 - policy updates 50/50 (current batch 500) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0002]
2025-10-23 17:02:43,193 | INFO | [MA-SPI] Stage 4/5 - completed
2025-10-23 17:02:43,193 | INFO | [MA-SPI] Stage 5/5 - collecting trajectories...
2025-10-23 17:02:43,374 | INFO | [MA-SPI] Stage 5/5 - collected 1/5 episodes
2025-10-23 17:02:43,514 | INFO | [MA-SPI] Stage 5/5 - collected 2/5 episodes
2025-10-23 17:02:43,643 | INFO | [MA-SPI] Stage 5/5 - collected 3/5 episodes
2025-10-23 17:02:43,788 | INFO | [MA-SPI] Stage 5/5 - collected 4/5 episodes
2025-10-23 17:02:43,926 | INFO | [MA-SPI] Stage 5/5 - collected 5/5 episodes
2025-10-23 17:02:43,927 | INFO | [MA-SPI] Stage 5/5 - Agent 0 avg episode reward: 121.800 (std 19.436)
2025-10-23 17:02:43,927 | INFO | [MA-SPI] Stage 5/5 - Agent 1 avg episode reward: 110.400 (std 17.984)
2025-10-23 17:02:43,927 | INFO | [MA-SPI] Stage 5/5 - dataset size: 500 transitions
2025-10-23 17:02:44,016 | INFO | [MA-SPI] Stage 5/5 - Q updates 20/100 (current batch 500) [A0_Q_loss:0.6875, A1_Q_loss:0.5534]
2025-10-23 17:02:44,106 | INFO | [MA-SPI] Stage 5/5 - Q updates 40/100 (current batch 500) [A0_Q_loss:0.5883, A1_Q_loss:0.4520]
2025-10-23 17:02:44,189 | INFO | [MA-SPI] Stage 5/5 - Q updates 60/100 (current batch 500) [A0_Q_loss:0.5703, A1_Q_loss:0.3922]
2025-10-23 17:02:44,276 | INFO | [MA-SPI] Stage 5/5 - Q updates 80/100 (current batch 500) [A0_Q_loss:0.5716, A1_Q_loss:0.3564]
2025-10-23 17:02:44,364 | INFO | [MA-SPI] Stage 5/5 - Q updates 100/100 (current batch 500) [A0_Q_loss:0.5434, A1_Q_loss:0.3597]
2025-10-23 17:02:44,404 | INFO | [MA-SPI] Stage 5/5 - policy updates 10/50 (current batch 500) [A0_Policy_loss:0.0052, A1_Policy_loss:0.0074]
2025-10-23 17:02:44,440 | INFO | [MA-SPI] Stage 5/5 - policy updates 20/50 (current batch 500) [A0_Policy_loss:0.0015, A1_Policy_loss:0.0024]
2025-10-23 17:02:44,485 | INFO | [MA-SPI] Stage 5/5 - policy updates 30/50 (current batch 500) [A0_Policy_loss:0.0005, A1_Policy_loss:0.0008]
2025-10-23 17:02:44,529 | INFO | [MA-SPI] Stage 5/5 - policy updates 40/50 (current batch 500) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0004]
2025-10-23 17:02:44,565 | INFO | [MA-SPI] Stage 5/5 - policy updates 50/50 (current batch 500) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0002]
2025-10-23 17:02:44,565 | INFO | [MA-SPI] Stage 5/5 - completed
2025-10-23 17:02:44,588 | INFO | [Experiment:heterogeneous] MA-SPI completed. Artifacts saved to outputs\heterogeneous\ma_spi
2025-10-23 17:02:44,588 | INFO | [Experiment:heterogeneous] Starting MA-LfL...
2025-10-23 17:02:44,588 | INFO | [MA-LfL] Estimating policies...
2025-10-23 17:02:44,588 | INFO | [MA-LfL][Policy] Stage 1/5 - preparing data
2025-10-23 17:02:44,624 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 0: 10/10 epochs completed (loss=1.0274, baseline=1.6094)
2025-10-23 17:02:44,646 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 1: 10/10 epochs completed (loss=0.9790, baseline=1.6094)
2025-10-23 17:02:44,646 | INFO | [MA-LfL][Policy] Stage 2/5 - preparing data
2025-10-23 17:02:44,665 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 0: 10/10 epochs completed (loss=0.8905, baseline=1.6094)
2025-10-23 17:02:44,696 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 1: 10/10 epochs completed (loss=0.9133, baseline=1.6094)
2025-10-23 17:02:44,696 | INFO | [MA-LfL][Policy] Stage 3/5 - preparing data
2025-10-23 17:02:44,714 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 0: 10/10 epochs completed (loss=0.9657, baseline=1.6094)
2025-10-23 17:02:44,739 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 1: 10/10 epochs completed (loss=0.9120, baseline=1.6094)
2025-10-23 17:02:44,739 | INFO | [MA-LfL][Policy] Stage 4/5 - preparing data
2025-10-23 17:02:44,759 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 0: 10/10 epochs completed (loss=0.9427, baseline=1.6094)
2025-10-23 17:02:44,779 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 1: 10/10 epochs completed (loss=0.9716, baseline=1.6094)
2025-10-23 17:02:44,780 | INFO | [MA-LfL][Policy] Stage 5/5 - preparing data
2025-10-23 17:02:44,800 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 0: 10/10 epochs completed (loss=0.9552, baseline=1.6094)
2025-10-23 17:02:44,830 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 1: 10/10 epochs completed (loss=1.0320, baseline=1.6094)
2025-10-23 17:02:44,832 | INFO | [MA-LfL] Computing reward targets...
2025-10-23 17:02:44,832 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 0
2025-10-23 17:02:45,216 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 1
2025-10-23 17:02:45,621 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 0
2025-10-23 17:02:46,030 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 1
2025-10-23 17:02:46,427 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 0
2025-10-23 17:02:46,833 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 1
2025-10-23 17:02:47,253 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 0
2025-10-23 17:02:47,659 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 1
2025-10-23 17:02:48,069 | INFO | [MA-LfL] Learning rewards...
2025-10-23 17:02:48,075 | INFO | [MA-LfL][Reward] Agent 0 - dataset size=2000 (batch=2000, epochs=10000)
2025-10-23 17:02:58,564 | INFO | [MA-LfL][Reward] Agent 0 - 1000/10000 epochs completed (latest loss=1.1693)
2025-10-23 17:03:08,838 | INFO | [MA-LfL][Reward] Agent 0 - 2000/10000 epochs completed (latest loss=1.1723)
2025-10-23 17:03:18,954 | INFO | [MA-LfL][Reward] Agent 0 - 3000/10000 epochs completed (latest loss=1.1687)
2025-10-23 17:03:29,421 | INFO | [MA-LfL][Reward] Agent 0 - 4000/10000 epochs completed (latest loss=1.1687)
2025-10-23 17:03:40,887 | INFO | [MA-LfL][Reward] Agent 0 - 5000/10000 epochs completed (latest loss=1.1687)
2025-10-23 17:03:51,880 | INFO | [MA-LfL][Reward] Agent 0 - 6000/10000 epochs completed (latest loss=1.2347)
2025-10-23 17:04:03,620 | INFO | [MA-LfL][Reward] Agent 0 - 7000/10000 epochs completed (latest loss=1.1687)
2025-10-23 17:04:13,863 | INFO | [MA-LfL][Reward] Agent 0 - 8000/10000 epochs completed (latest loss=1.1687)
2025-10-23 17:04:24,475 | INFO | [MA-LfL][Reward] Agent 0 - 9000/10000 epochs completed (latest loss=1.1687)
2025-10-23 17:04:34,266 | INFO | [MA-LfL][Reward] Agent 0 - 10000/10000 epochs completed (latest loss=1.1690)
2025-10-23 17:04:34,271 | INFO | [MA-LfL][Reward] Agent 1 - dataset size=2000 (batch=2000, epochs=10000)
2025-10-23 17:04:44,340 | INFO | [MA-LfL][Reward] Agent 1 - 1000/10000 epochs completed (latest loss=1.1865)
2025-10-23 17:04:55,518 | INFO | [MA-LfL][Reward] Agent 1 - 2000/10000 epochs completed (latest loss=1.1202)
2025-10-23 17:05:05,747 | INFO | [MA-LfL][Reward] Agent 1 - 3000/10000 epochs completed (latest loss=1.1208)
2025-10-23 17:05:16,017 | INFO | [MA-LfL][Reward] Agent 1 - 4000/10000 epochs completed (latest loss=1.1295)
2025-10-23 17:05:26,419 | INFO | [MA-LfL][Reward] Agent 1 - 5000/10000 epochs completed (latest loss=1.1326)
2025-10-23 17:05:37,844 | INFO | [MA-LfL][Reward] Agent 1 - 6000/10000 epochs completed (latest loss=1.1200)
2025-10-23 17:05:48,294 | INFO | [MA-LfL][Reward] Agent 1 - 7000/10000 epochs completed (latest loss=1.1200)
2025-10-23 17:05:58,538 | INFO | [MA-LfL][Reward] Agent 1 - 8000/10000 epochs completed (latest loss=1.1206)
2025-10-23 17:06:08,957 | INFO | [MA-LfL][Reward] Agent 1 - 9000/10000 epochs completed (latest loss=1.1200)
2025-10-23 17:06:20,936 | INFO | [MA-LfL][Reward] Agent 1 - 10000/10000 epochs completed (latest loss=1.1199)
2025-10-23 17:06:20,937 | INFO | [MA-LfL] Evaluating rewards...
2025-10-23 17:06:21,698 | INFO | [MA-LfL] Pipeline complete.
2025-10-23 17:06:21,698 | INFO | [Experiment:heterogeneous] MA-LfL completed. Artifacts saved to outputs\heterogeneous\ma_lfl
2025-10-23 17:06:21,698 | INFO | [Experiment:heterogeneous] Agent 0 metrics - Pearson: -0.0448 | Spearman: 0.0226
2025-10-23 17:06:21,698 | INFO | [Experiment:heterogeneous] Agent 1 metrics - Pearson: 0.1370 | Spearman: 0.1195
2025-10-23 17:06:21,698 | INFO | [Experiment:heterogeneous] Trend stages: 1, 2, 3, 4
2025-10-23 17:06:21,699 | INFO | [Experiment:heterogeneous] Trend Pearson: 0.1953, 0.6239, 0.6408, 0.6428
2025-10-23 17:06:21,699 | INFO | [Experiment:heterogeneous] Trend Spearman: 0.2418, 0.6955, 0.7361, 0.6624
2025-10-23 17:06:21,699 | INFO | [Experiment:heterogeneous] Agent 0 correlation >=0.4 ? NO
2025-10-23 17:06:21,699 | INFO | [Experiment:heterogeneous] Agent 1 correlation >=0.4 ? NO
2025-10-23 17:06:21,699 | INFO | [Experiment:heterogeneous] Correlation trend non-decreasing? NO
2025-10-23 17:06:21,699 | INFO | Summary saved to outputs\summary.json
2025-10-23 17:06:21,700 | INFO | Run complete.
