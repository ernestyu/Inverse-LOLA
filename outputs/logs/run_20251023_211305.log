2025-10-23 21:13:05,072 | INFO | Logging initialized. Log file: outputs\logs\run_20251023_211305.log
2025-10-23 21:13:05,072 | INFO | Configuration file: config.yaml
2025-10-23 21:13:05,072 | INFO | Reward families to process: heterogeneous
2025-10-23 21:13:05,072 | INFO | MA-SPI iterations=5, episodes/stage=10, episode_length=100
2025-10-23 21:13:05,074 | INFO | MA-LfL reward epochs=20000, reward batch size=8192
2025-10-23 21:13:05,074 | INFO | Log file located at outputs\logs\run_20251023_211305.log
2025-10-23 21:13:05,074 | INFO | Running experiment for reward family: heterogeneous
2025-10-23 21:13:05,074 | INFO | [Experiment:heterogeneous] Starting MA-SPI...
2025-10-23 21:13:05,938 | INFO | [MA-SPI] Starting run with 5 stages, 10 episodes/stage, episode length 100
2025-10-23 21:13:05,950 | INFO | [MA-SPI] Stage 1/5 - collecting trajectories...
2025-10-23 21:13:06,372 | INFO | [MA-SPI] Stage 1/5 - collected 2/10 episodes
2025-10-23 21:13:06,629 | INFO | [MA-SPI] Stage 1/5 - collected 4/10 episodes
2025-10-23 21:13:06,877 | INFO | [MA-SPI] Stage 1/5 - collected 6/10 episodes
2025-10-23 21:13:07,138 | INFO | [MA-SPI] Stage 1/5 - collected 8/10 episodes
2025-10-23 21:13:07,426 | INFO | [MA-SPI] Stage 1/5 - collected 10/10 episodes
2025-10-23 21:13:07,427 | INFO | [MA-SPI] Stage 1/5 - Agent 0 avg episode reward: -137.400 (std 33.539)
2025-10-23 21:13:07,427 | INFO | [MA-SPI] Stage 1/5 - Agent 1 avg episode reward: -114.900 (std 35.184)
2025-10-23 21:13:07,427 | INFO | [MA-SPI] Stage 1/5 - dataset size: 1000 transitions
2025-10-23 21:13:07,661 | INFO | [MA-SPI] Stage 1/5 - Q updates 20/100 (current batch 1000) [A0_Q_loss:2.7630, A1_Q_loss:3.1812]
2025-10-23 21:13:07,752 | INFO | [MA-SPI] Stage 1/5 - Q updates 40/100 (current batch 1000) [A0_Q_loss:1.3751, A1_Q_loss:1.4296]
2025-10-23 21:13:07,835 | INFO | [MA-SPI] Stage 1/5 - Q updates 60/100 (current batch 1000) [A0_Q_loss:0.8376, A1_Q_loss:1.0367]
2025-10-23 21:13:07,915 | INFO | [MA-SPI] Stage 1/5 - Q updates 80/100 (current batch 1000) [A0_Q_loss:0.5093, A1_Q_loss:0.7529]
2025-10-23 21:13:07,997 | INFO | [MA-SPI] Stage 1/5 - Q updates 100/100 (current batch 1000) [A0_Q_loss:0.4704, A1_Q_loss:0.5672]
2025-10-23 21:13:08,032 | INFO | [MA-SPI] Stage 1/5 - policy updates 10/50 (current batch 1000) [A0_Policy_loss:0.0288, A1_Policy_loss:0.0183]
2025-10-23 21:13:08,069 | INFO | [MA-SPI] Stage 1/5 - policy updates 20/50 (current batch 1000) [A0_Policy_loss:0.0090, A1_Policy_loss:0.0042]
2025-10-23 21:13:08,106 | INFO | [MA-SPI] Stage 1/5 - policy updates 30/50 (current batch 1000) [A0_Policy_loss:0.0034, A1_Policy_loss:0.0015]
2025-10-23 21:13:08,139 | INFO | [MA-SPI] Stage 1/5 - policy updates 40/50 (current batch 1000) [A0_Policy_loss:0.0013, A1_Policy_loss:0.0006]
2025-10-23 21:13:08,175 | INFO | [MA-SPI] Stage 1/5 - policy updates 50/50 (current batch 1000) [A0_Policy_loss:0.0005, A1_Policy_loss:0.0002]
2025-10-23 21:13:08,175 | INFO | [MA-SPI] Stage 1/5 - completed
2025-10-23 21:13:08,175 | INFO | [MA-SPI] Stage 2/5 - collecting trajectories...
2025-10-23 21:13:08,466 | INFO | [MA-SPI] Stage 2/5 - collected 2/10 episodes
2025-10-23 21:13:08,730 | INFO | [MA-SPI] Stage 2/5 - collected 4/10 episodes
2025-10-23 21:13:08,994 | INFO | [MA-SPI] Stage 2/5 - collected 6/10 episodes
2025-10-23 21:13:09,261 | INFO | [MA-SPI] Stage 2/5 - collected 8/10 episodes
2025-10-23 21:13:09,510 | INFO | [MA-SPI] Stage 2/5 - collected 10/10 episodes
2025-10-23 21:13:09,511 | INFO | [MA-SPI] Stage 2/5 - Agent 0 avg episode reward: 67.000 (std 29.783)
2025-10-23 21:13:09,512 | INFO | [MA-SPI] Stage 2/5 - Agent 1 avg episode reward: 47.100 (std 20.535)
2025-10-23 21:13:09,512 | INFO | [MA-SPI] Stage 2/5 - dataset size: 1000 transitions
2025-10-23 21:13:09,595 | INFO | [MA-SPI] Stage 2/5 - Q updates 20/100 (current batch 1000) [A0_Q_loss:1.6607, A1_Q_loss:1.4316]
2025-10-23 21:13:09,675 | INFO | [MA-SPI] Stage 2/5 - Q updates 40/100 (current batch 1000) [A0_Q_loss:1.0833, A1_Q_loss:1.1384]
2025-10-23 21:13:09,763 | INFO | [MA-SPI] Stage 2/5 - Q updates 60/100 (current batch 1000) [A0_Q_loss:0.9357, A1_Q_loss:1.0149]
2025-10-23 21:13:09,847 | INFO | [MA-SPI] Stage 2/5 - Q updates 80/100 (current batch 1000) [A0_Q_loss:0.8746, A1_Q_loss:0.9769]
2025-10-23 21:13:09,928 | INFO | [MA-SPI] Stage 2/5 - Q updates 100/100 (current batch 1000) [A0_Q_loss:0.8530, A1_Q_loss:0.9853]
2025-10-23 21:13:09,962 | INFO | [MA-SPI] Stage 2/5 - policy updates 10/50 (current batch 1000) [A0_Policy_loss:0.0158, A1_Policy_loss:0.0072]
2025-10-23 21:13:09,995 | INFO | [MA-SPI] Stage 2/5 - policy updates 20/50 (current batch 1000) [A0_Policy_loss:0.0048, A1_Policy_loss:0.0025]
2025-10-23 21:13:10,027 | INFO | [MA-SPI] Stage 2/5 - policy updates 30/50 (current batch 1000) [A0_Policy_loss:0.0020, A1_Policy_loss:0.0010]
2025-10-23 21:13:10,061 | INFO | [MA-SPI] Stage 2/5 - policy updates 40/50 (current batch 1000) [A0_Policy_loss:0.0006, A1_Policy_loss:0.0004]
2025-10-23 21:13:10,100 | INFO | [MA-SPI] Stage 2/5 - policy updates 50/50 (current batch 1000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0002]
2025-10-23 21:13:10,100 | INFO | [MA-SPI] Stage 2/5 - completed
2025-10-23 21:13:10,100 | INFO | [MA-SPI] Stage 3/5 - collecting trajectories...
2025-10-23 21:13:10,361 | INFO | [MA-SPI] Stage 3/5 - collected 2/10 episodes
2025-10-23 21:13:10,602 | INFO | [MA-SPI] Stage 3/5 - collected 4/10 episodes
2025-10-23 21:13:10,856 | INFO | [MA-SPI] Stage 3/5 - collected 6/10 episodes
2025-10-23 21:13:11,097 | INFO | [MA-SPI] Stage 3/5 - collected 8/10 episodes
2025-10-23 21:13:11,363 | INFO | [MA-SPI] Stage 3/5 - collected 10/10 episodes
2025-10-23 21:13:11,365 | INFO | [MA-SPI] Stage 3/5 - Agent 0 avg episode reward: 113.400 (std 28.051)
2025-10-23 21:13:11,365 | INFO | [MA-SPI] Stage 3/5 - Agent 1 avg episode reward: 109.100 (std 25.137)
2025-10-23 21:13:11,365 | INFO | [MA-SPI] Stage 3/5 - dataset size: 1000 transitions
2025-10-23 21:13:11,447 | INFO | [MA-SPI] Stage 3/5 - Q updates 20/100 (current batch 1000) [A0_Q_loss:0.7991, A1_Q_loss:0.6151]
2025-10-23 21:13:11,526 | INFO | [MA-SPI] Stage 3/5 - Q updates 40/100 (current batch 1000) [A0_Q_loss:0.8251, A1_Q_loss:0.5328]
2025-10-23 21:13:11,606 | INFO | [MA-SPI] Stage 3/5 - Q updates 60/100 (current batch 1000) [A0_Q_loss:0.7463, A1_Q_loss:0.5244]
2025-10-23 21:13:11,684 | INFO | [MA-SPI] Stage 3/5 - Q updates 80/100 (current batch 1000) [A0_Q_loss:0.7612, A1_Q_loss:0.5156]
2025-10-23 21:13:11,764 | INFO | [MA-SPI] Stage 3/5 - Q updates 100/100 (current batch 1000) [A0_Q_loss:0.7643, A1_Q_loss:0.5197]
2025-10-23 21:13:11,800 | INFO | [MA-SPI] Stage 3/5 - policy updates 10/50 (current batch 1000) [A0_Policy_loss:0.0067, A1_Policy_loss:0.0051]
2025-10-23 21:13:11,837 | INFO | [MA-SPI] Stage 3/5 - policy updates 20/50 (current batch 1000) [A0_Policy_loss:0.0023, A1_Policy_loss:0.0019]
2025-10-23 21:13:11,872 | INFO | [MA-SPI] Stage 3/5 - policy updates 30/50 (current batch 1000) [A0_Policy_loss:0.0008, A1_Policy_loss:0.0007]
2025-10-23 21:13:11,905 | INFO | [MA-SPI] Stage 3/5 - policy updates 40/50 (current batch 1000) [A0_Policy_loss:0.0003, A1_Policy_loss:0.0002]
2025-10-23 21:13:11,940 | INFO | [MA-SPI] Stage 3/5 - policy updates 50/50 (current batch 1000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0001]
2025-10-23 21:13:11,940 | INFO | [MA-SPI] Stage 3/5 - completed
2025-10-23 21:13:11,941 | INFO | [MA-SPI] Stage 4/5 - collecting trajectories...
2025-10-23 21:13:12,212 | INFO | [MA-SPI] Stage 4/5 - collected 2/10 episodes
2025-10-23 21:13:12,481 | INFO | [MA-SPI] Stage 4/5 - collected 4/10 episodes
2025-10-23 21:13:12,731 | INFO | [MA-SPI] Stage 4/5 - collected 6/10 episodes
2025-10-23 21:13:12,992 | INFO | [MA-SPI] Stage 4/5 - collected 8/10 episodes
2025-10-23 21:13:13,250 | INFO | [MA-SPI] Stage 4/5 - collected 10/10 episodes
2025-10-23 21:13:13,251 | INFO | [MA-SPI] Stage 4/5 - Agent 0 avg episode reward: 112.800 (std 30.225)
2025-10-23 21:13:13,251 | INFO | [MA-SPI] Stage 4/5 - Agent 1 avg episode reward: 98.900 (std 33.721)
2025-10-23 21:13:13,251 | INFO | [MA-SPI] Stage 4/5 - dataset size: 1000 transitions
2025-10-23 21:13:13,341 | INFO | [MA-SPI] Stage 4/5 - Q updates 20/100 (current batch 1000) [A0_Q_loss:0.5955, A1_Q_loss:0.4295]
2025-10-23 21:13:13,422 | INFO | [MA-SPI] Stage 4/5 - Q updates 40/100 (current batch 1000) [A0_Q_loss:0.5255, A1_Q_loss:0.3559]
2025-10-23 21:13:13,501 | INFO | [MA-SPI] Stage 4/5 - Q updates 60/100 (current batch 1000) [A0_Q_loss:0.4722, A1_Q_loss:0.3525]
2025-10-23 21:13:13,584 | INFO | [MA-SPI] Stage 4/5 - Q updates 80/100 (current batch 1000) [A0_Q_loss:0.5347, A1_Q_loss:0.3298]
2025-10-23 21:13:13,661 | INFO | [MA-SPI] Stage 4/5 - Q updates 100/100 (current batch 1000) [A0_Q_loss:0.4735, A1_Q_loss:0.3408]
2025-10-23 21:13:13,693 | INFO | [MA-SPI] Stage 4/5 - policy updates 10/50 (current batch 1000) [A0_Policy_loss:0.0037, A1_Policy_loss:0.0032]
2025-10-23 21:13:13,724 | INFO | [MA-SPI] Stage 4/5 - policy updates 20/50 (current batch 1000) [A0_Policy_loss:0.0011, A1_Policy_loss:0.0010]
2025-10-23 21:13:13,757 | INFO | [MA-SPI] Stage 4/5 - policy updates 30/50 (current batch 1000) [A0_Policy_loss:0.0005, A1_Policy_loss:0.0005]
2025-10-23 21:13:13,789 | INFO | [MA-SPI] Stage 4/5 - policy updates 40/50 (current batch 1000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0002]
2025-10-23 21:13:13,822 | INFO | [MA-SPI] Stage 4/5 - policy updates 50/50 (current batch 1000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0001]
2025-10-23 21:13:13,823 | INFO | [MA-SPI] Stage 4/5 - completed
2025-10-23 21:13:13,823 | INFO | [MA-SPI] Stage 5/5 - collecting trajectories...
2025-10-23 21:13:14,096 | INFO | [MA-SPI] Stage 5/5 - collected 2/10 episodes
2025-10-23 21:13:14,367 | INFO | [MA-SPI] Stage 5/5 - collected 4/10 episodes
2025-10-23 21:13:14,650 | INFO | [MA-SPI] Stage 5/5 - collected 6/10 episodes
2025-10-23 21:13:14,922 | INFO | [MA-SPI] Stage 5/5 - collected 8/10 episodes
2025-10-23 21:13:15,199 | INFO | [MA-SPI] Stage 5/5 - collected 10/10 episodes
2025-10-23 21:13:15,200 | INFO | [MA-SPI] Stage 5/5 - Agent 0 avg episode reward: 115.500 (std 18.720)
2025-10-23 21:13:15,201 | INFO | [MA-SPI] Stage 5/5 - Agent 1 avg episode reward: 110.000 (std 23.422)
2025-10-23 21:13:15,201 | INFO | [MA-SPI] Stage 5/5 - dataset size: 1000 transitions
2025-10-23 21:13:15,282 | INFO | [MA-SPI] Stage 5/5 - Q updates 20/100 (current batch 1000) [A0_Q_loss:0.5018, A1_Q_loss:0.4675]
2025-10-23 21:13:15,363 | INFO | [MA-SPI] Stage 5/5 - Q updates 40/100 (current batch 1000) [A0_Q_loss:0.4171, A1_Q_loss:0.4131]
2025-10-23 21:13:15,440 | INFO | [MA-SPI] Stage 5/5 - Q updates 60/100 (current batch 1000) [A0_Q_loss:0.4285, A1_Q_loss:0.3537]
2025-10-23 21:13:15,518 | INFO | [MA-SPI] Stage 5/5 - Q updates 80/100 (current batch 1000) [A0_Q_loss:0.4103, A1_Q_loss:0.4335]
2025-10-23 21:13:15,596 | INFO | [MA-SPI] Stage 5/5 - Q updates 100/100 (current batch 1000) [A0_Q_loss:0.4163, A1_Q_loss:0.4042]
2025-10-23 21:13:15,628 | INFO | [MA-SPI] Stage 5/5 - policy updates 10/50 (current batch 1000) [A0_Policy_loss:0.0023, A1_Policy_loss:0.0031]
2025-10-23 21:13:15,661 | INFO | [MA-SPI] Stage 5/5 - policy updates 20/50 (current batch 1000) [A0_Policy_loss:0.0007, A1_Policy_loss:0.0012]
2025-10-23 21:13:15,694 | INFO | [MA-SPI] Stage 5/5 - policy updates 30/50 (current batch 1000) [A0_Policy_loss:0.0003, A1_Policy_loss:0.0005]
2025-10-23 21:13:15,727 | INFO | [MA-SPI] Stage 5/5 - policy updates 40/50 (current batch 1000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0002]
2025-10-23 21:13:15,763 | INFO | [MA-SPI] Stage 5/5 - policy updates 50/50 (current batch 1000) [A0_Policy_loss:0.0000, A1_Policy_loss:0.0001]
2025-10-23 21:13:15,763 | INFO | [MA-SPI] Stage 5/5 - completed
2025-10-23 21:13:15,793 | INFO | [Experiment:heterogeneous] MA-SPI completed. Artifacts saved to outputs\heterogeneous\ma_spi
2025-10-23 21:13:15,794 | INFO | [Experiment:heterogeneous] Starting MA-LfL...
2025-10-23 21:13:15,794 | INFO | [MA-LfL] Estimating policies...
2025-10-23 21:13:15,794 | INFO | [MA-LfL][Policy] Stage 1/5 - preparing data
2025-10-23 21:13:15,831 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 0: 10/10 epochs completed (loss=1.0913, baseline=1.6094)
2025-10-23 21:13:15,850 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 1: 10/10 epochs completed (loss=1.0798, baseline=1.6094)
2025-10-23 21:13:15,851 | INFO | [MA-LfL][Policy] Stage 2/5 - preparing data
2025-10-23 21:13:15,873 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 0: 10/10 epochs completed (loss=0.9976, baseline=1.6094)
2025-10-23 21:13:15,892 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 1: 10/10 epochs completed (loss=0.9760, baseline=1.6094)
2025-10-23 21:13:15,892 | INFO | [MA-LfL][Policy] Stage 3/5 - preparing data
2025-10-23 21:13:15,915 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 0: 10/10 epochs completed (loss=0.9835, baseline=1.6094)
2025-10-23 21:13:15,936 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 1: 10/10 epochs completed (loss=1.0029, baseline=1.6094)
2025-10-23 21:13:15,937 | INFO | [MA-LfL][Policy] Stage 4/5 - preparing data
2025-10-23 21:13:15,958 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 0: 10/10 epochs completed (loss=1.0035, baseline=1.6094)
2025-10-23 21:13:15,978 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 1: 10/10 epochs completed (loss=1.0177, baseline=1.6094)
2025-10-23 21:13:15,978 | INFO | [MA-LfL][Policy] Stage 5/5 - preparing data
2025-10-23 21:13:16,002 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 0: 10/10 epochs completed (loss=1.0083, baseline=1.6094)
2025-10-23 21:13:16,025 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 1: 10/10 epochs completed (loss=1.0069, baseline=1.6094)
2025-10-23 21:13:16,025 | INFO | [MA-LfL] Computing reward targets...
2025-10-23 21:13:16,025 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 0
2025-10-23 21:13:16,399 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 1
2025-10-23 21:13:16,742 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 0
2025-10-23 21:13:17,054 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 1
2025-10-23 21:13:17,393 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 0
2025-10-23 21:13:17,756 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 1
2025-10-23 21:13:18,144 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 0
2025-10-23 21:13:18,526 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 1
2025-10-23 21:13:18,909 | INFO | [MA-LfL] Learning rewards...
2025-10-23 21:13:18,916 | INFO | [MA-LfL][Reward] Agent 0 - dataset size=4000 (batch=4000, epochs=20000)
2025-10-23 21:13:40,404 | INFO | [MA-LfL][Reward] Agent 0 - 2000/20000 epochs completed (latest loss=0.8538)
2025-10-23 21:14:02,200 | INFO | [MA-LfL][Reward] Agent 0 - 4000/20000 epochs completed (latest loss=0.8538)
2025-10-23 21:14:25,783 | INFO | [MA-LfL][Reward] Agent 0 - 6000/20000 epochs completed (latest loss=0.8538)
2025-10-23 21:14:49,436 | INFO | [MA-LfL][Reward] Agent 0 - 8000/20000 epochs completed (latest loss=0.8539)
2025-10-23 21:15:14,415 | INFO | [MA-LfL][Reward] Agent 0 - 10000/20000 epochs completed (latest loss=0.8538)
2025-10-23 21:15:38,387 | INFO | [MA-LfL][Reward] Agent 0 - 12000/20000 epochs completed (latest loss=0.8538)
2025-10-23 21:16:02,675 | INFO | [MA-LfL][Reward] Agent 0 - 14000/20000 epochs completed (latest loss=0.8538)
2025-10-23 21:16:26,455 | INFO | [MA-LfL][Reward] Agent 0 - 16000/20000 epochs completed (latest loss=0.8541)
2025-10-23 21:16:52,096 | INFO | [MA-LfL][Reward] Agent 0 - 18000/20000 epochs completed (latest loss=0.8538)
2025-10-23 21:17:15,943 | INFO | [MA-LfL][Reward] Agent 0 - 20000/20000 epochs completed (latest loss=0.8538)
2025-10-23 21:17:15,950 | INFO | [MA-LfL][Reward] Agent 1 - dataset size=4000 (batch=4000, epochs=20000)
2025-10-23 21:17:40,167 | INFO | [MA-LfL][Reward] Agent 1 - 2000/20000 epochs completed (latest loss=0.5375)
2025-10-23 21:18:05,097 | INFO | [MA-LfL][Reward] Agent 1 - 4000/20000 epochs completed (latest loss=0.5375)
2025-10-23 21:18:28,947 | INFO | [MA-LfL][Reward] Agent 1 - 6000/20000 epochs completed (latest loss=0.5375)
2025-10-23 21:18:52,548 | INFO | [MA-LfL][Reward] Agent 1 - 8000/20000 epochs completed (latest loss=0.5375)
2025-10-23 21:19:17,887 | INFO | [MA-LfL][Reward] Agent 1 - 10000/20000 epochs completed (latest loss=0.5375)
2025-10-23 21:19:39,976 | INFO | [MA-LfL][Reward] Agent 1 - 12000/20000 epochs completed (latest loss=0.5375)
2025-10-23 21:20:02,918 | INFO | [MA-LfL][Reward] Agent 1 - 14000/20000 epochs completed (latest loss=0.5375)
2025-10-23 21:20:25,793 | INFO | [MA-LfL][Reward] Agent 1 - 16000/20000 epochs completed (latest loss=0.5375)
2025-10-23 21:20:49,040 | INFO | [MA-LfL][Reward] Agent 1 - 18000/20000 epochs completed (latest loss=0.5375)
2025-10-23 21:21:11,909 | INFO | [MA-LfL][Reward] Agent 1 - 20000/20000 epochs completed (latest loss=0.5375)
2025-10-23 21:21:11,909 | INFO | [MA-LfL] Evaluating rewards...
2025-10-23 21:21:12,733 | INFO | [MA-LfL] Pipeline complete.
2025-10-23 21:21:12,734 | INFO | [Experiment:heterogeneous] MA-LfL completed. Artifacts saved to outputs\heterogeneous\ma_lfl
2025-10-23 21:21:12,734 | INFO | [Experiment:heterogeneous] Agent 0 metrics - Pearson: 0.2722 | Spearman: 0.2758
2025-10-23 21:21:12,734 | INFO | [Experiment:heterogeneous] Agent 1 metrics - Pearson: 0.0468 | Spearman: 0.0840
2025-10-23 21:21:12,734 | INFO | [Experiment:heterogeneous] Trend stages: 1, 2, 3, 4
2025-10-23 21:21:12,735 | INFO | [Experiment:heterogeneous] Trend Pearson: 0.4034, 0.6917, 0.5595, 0.5787
2025-10-23 21:21:12,735 | INFO | [Experiment:heterogeneous] Trend Spearman: 0.4059, 0.6826, 0.6808, 0.5968
2025-10-23 21:21:12,735 | INFO | [Experiment:heterogeneous] Agent 0 correlation >=0.4 ? NO
2025-10-23 21:21:12,735 | INFO | [Experiment:heterogeneous] Agent 1 correlation >=0.4 ? NO
2025-10-23 21:21:12,735 | INFO | [Experiment:heterogeneous] Correlation trend non-decreasing? NO
2025-10-23 21:21:12,736 | INFO | Summary saved to outputs\summary.json
2025-10-23 21:21:12,736 | INFO | Run complete.
