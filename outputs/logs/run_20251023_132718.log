2025-10-23 13:27:18,518 | INFO | Logging initialized. Log file: outputs\logs\run_20251023_132718.log
2025-10-23 13:27:18,519 | INFO | Configuration file: config.yaml
2025-10-23 13:27:18,519 | INFO | Reward families to process: heterogeneous
2025-10-23 13:27:18,519 | INFO | MA-SPI iterations=5, episodes/stage=30, episode_length=100
2025-10-23 13:27:18,520 | INFO | MA-LfL reward epochs=10000, reward batch size=8192
2025-10-23 13:27:18,520 | INFO | Log file located at outputs\logs\run_20251023_132718.log
2025-10-23 13:27:18,520 | INFO | Running experiment for reward family: heterogeneous
2025-10-23 13:27:18,520 | INFO | [Experiment:heterogeneous] Starting MA-SPI...
2025-10-23 13:27:19,554 | INFO | [MA-SPI] Starting run with 5 stages, 30 episodes/stage, episode length 100
2025-10-23 13:27:19,565 | INFO | [MA-SPI] Stage 1/5 - collecting trajectories...
2025-10-23 13:27:20,682 | INFO | [MA-SPI] Stage 1/5 - collected 6/30 episodes
2025-10-23 13:27:21,603 | INFO | [MA-SPI] Stage 1/5 - collected 12/30 episodes
2025-10-23 13:27:22,429 | INFO | [MA-SPI] Stage 1/5 - collected 18/30 episodes
2025-10-23 13:27:23,289 | INFO | [MA-SPI] Stage 1/5 - collected 24/30 episodes
2025-10-23 13:27:24,126 | INFO | [MA-SPI] Stage 1/5 - collected 30/30 episodes
2025-10-23 13:27:24,128 | INFO | [MA-SPI] Stage 1/5 - Agent 0 avg episode reward: -133.567 (std 31.196)
2025-10-23 13:27:24,128 | INFO | [MA-SPI] Stage 1/5 - Agent 1 avg episode reward: -113.300 (std 31.693)
2025-10-23 13:27:24,128 | INFO | [MA-SPI] Stage 1/5 - dataset size: 3000 transitions
2025-10-23 13:27:24,367 | INFO | [MA-SPI] Stage 1/5 - Q updates 20/100 (current batch 3000) [A0_Q_loss:3.0577, A1_Q_loss:3.0181]
2025-10-23 13:27:24,461 | INFO | [MA-SPI] Stage 1/5 - Q updates 40/100 (current batch 3000) [A0_Q_loss:1.5579, A1_Q_loss:1.4081]
2025-10-23 13:27:24,556 | INFO | [MA-SPI] Stage 1/5 - Q updates 60/100 (current batch 3000) [A0_Q_loss:1.1305, A1_Q_loss:0.7885]
2025-10-23 13:27:24,649 | INFO | [MA-SPI] Stage 1/5 - Q updates 80/100 (current batch 3000) [A0_Q_loss:0.7832, A1_Q_loss:0.6657]
2025-10-23 13:27:24,739 | INFO | [MA-SPI] Stage 1/5 - Q updates 100/100 (current batch 3000) [A0_Q_loss:0.6631, A1_Q_loss:0.6067]
2025-10-23 13:27:24,780 | INFO | [MA-SPI] Stage 1/5 - policy updates 10/50 (current batch 3000) [A0_Policy_loss:0.0246, A1_Policy_loss:0.0224]
2025-10-23 13:27:24,820 | INFO | [MA-SPI] Stage 1/5 - policy updates 20/50 (current batch 3000) [A0_Policy_loss:0.0081, A1_Policy_loss:0.0053]
2025-10-23 13:27:24,857 | INFO | [MA-SPI] Stage 1/5 - policy updates 30/50 (current batch 3000) [A0_Policy_loss:0.0029, A1_Policy_loss:0.0016]
2025-10-23 13:27:24,896 | INFO | [MA-SPI] Stage 1/5 - policy updates 40/50 (current batch 3000) [A0_Policy_loss:0.0008, A1_Policy_loss:0.0006]
2025-10-23 13:27:24,935 | INFO | [MA-SPI] Stage 1/5 - policy updates 50/50 (current batch 3000) [A0_Policy_loss:0.0003, A1_Policy_loss:0.0002]
2025-10-23 13:27:24,935 | INFO | [MA-SPI] Stage 1/5 - completed
2025-10-23 13:27:24,935 | INFO | [MA-SPI] Stage 2/5 - collecting trajectories...
2025-10-23 13:27:25,740 | INFO | [MA-SPI] Stage 2/5 - collected 6/30 episodes
2025-10-23 13:27:26,580 | INFO | [MA-SPI] Stage 2/5 - collected 12/30 episodes
2025-10-23 13:27:27,398 | INFO | [MA-SPI] Stage 2/5 - collected 18/30 episodes
2025-10-23 13:27:28,292 | INFO | [MA-SPI] Stage 2/5 - collected 24/30 episodes
2025-10-23 13:27:29,128 | INFO | [MA-SPI] Stage 2/5 - collected 30/30 episodes
2025-10-23 13:27:29,131 | INFO | [MA-SPI] Stage 2/5 - Agent 0 avg episode reward: 54.900 (std 37.226)
2025-10-23 13:27:29,131 | INFO | [MA-SPI] Stage 2/5 - Agent 1 avg episode reward: 59.633 (std 35.896)
2025-10-23 13:27:29,131 | INFO | [MA-SPI] Stage 2/5 - dataset size: 3000 transitions
2025-10-23 13:27:29,230 | INFO | [MA-SPI] Stage 2/5 - Q updates 20/100 (current batch 3000) [A0_Q_loss:1.7503, A1_Q_loss:1.1955]
2025-10-23 13:27:29,328 | INFO | [MA-SPI] Stage 2/5 - Q updates 40/100 (current batch 3000) [A0_Q_loss:0.8051, A1_Q_loss:1.0462]
2025-10-23 13:27:29,428 | INFO | [MA-SPI] Stage 2/5 - Q updates 60/100 (current batch 3000) [A0_Q_loss:0.7919, A1_Q_loss:0.9814]
2025-10-23 13:27:29,514 | INFO | [MA-SPI] Stage 2/5 - Q updates 80/100 (current batch 3000) [A0_Q_loss:0.7450, A1_Q_loss:0.9524]
2025-10-23 13:27:29,604 | INFO | [MA-SPI] Stage 2/5 - Q updates 100/100 (current batch 3000) [A0_Q_loss:0.7436, A1_Q_loss:0.9499]
2025-10-23 13:27:29,640 | INFO | [MA-SPI] Stage 2/5 - policy updates 10/50 (current batch 3000) [A0_Policy_loss:0.0128, A1_Policy_loss:0.0057]
2025-10-23 13:27:29,676 | INFO | [MA-SPI] Stage 2/5 - policy updates 20/50 (current batch 3000) [A0_Policy_loss:0.0034, A1_Policy_loss:0.0028]
2025-10-23 13:27:29,711 | INFO | [MA-SPI] Stage 2/5 - policy updates 30/50 (current batch 3000) [A0_Policy_loss:0.0014, A1_Policy_loss:0.0008]
2025-10-23 13:27:29,748 | INFO | [MA-SPI] Stage 2/5 - policy updates 40/50 (current batch 3000) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0003]
2025-10-23 13:27:29,783 | INFO | [MA-SPI] Stage 2/5 - policy updates 50/50 (current batch 3000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0001]
2025-10-23 13:27:29,783 | INFO | [MA-SPI] Stage 2/5 - completed
2025-10-23 13:27:29,783 | INFO | [MA-SPI] Stage 3/5 - collecting trajectories...
2025-10-23 13:27:30,602 | INFO | [MA-SPI] Stage 3/5 - collected 6/30 episodes
2025-10-23 13:27:31,456 | INFO | [MA-SPI] Stage 3/5 - collected 12/30 episodes
2025-10-23 13:27:32,302 | INFO | [MA-SPI] Stage 3/5 - collected 18/30 episodes
2025-10-23 13:27:33,153 | INFO | [MA-SPI] Stage 3/5 - collected 24/30 episodes
2025-10-23 13:27:33,992 | INFO | [MA-SPI] Stage 3/5 - collected 30/30 episodes
2025-10-23 13:27:33,994 | INFO | [MA-SPI] Stage 3/5 - Agent 0 avg episode reward: 110.600 (std 18.804)
2025-10-23 13:27:33,994 | INFO | [MA-SPI] Stage 3/5 - Agent 1 avg episode reward: 106.733 (std 20.258)
2025-10-23 13:27:33,994 | INFO | [MA-SPI] Stage 3/5 - dataset size: 3000 transitions
2025-10-23 13:27:34,111 | INFO | [MA-SPI] Stage 3/5 - Q updates 20/100 (current batch 3000) [A0_Q_loss:0.6757, A1_Q_loss:0.5499]
2025-10-23 13:27:34,211 | INFO | [MA-SPI] Stage 3/5 - Q updates 40/100 (current batch 3000) [A0_Q_loss:0.6097, A1_Q_loss:0.3913]
2025-10-23 13:27:34,300 | INFO | [MA-SPI] Stage 3/5 - Q updates 60/100 (current batch 3000) [A0_Q_loss:0.6006, A1_Q_loss:0.3474]
2025-10-23 13:27:34,388 | INFO | [MA-SPI] Stage 3/5 - Q updates 80/100 (current batch 3000) [A0_Q_loss:0.5920, A1_Q_loss:0.3565]
2025-10-23 13:27:34,475 | INFO | [MA-SPI] Stage 3/5 - Q updates 100/100 (current batch 3000) [A0_Q_loss:0.6041, A1_Q_loss:0.3335]
2025-10-23 13:27:34,513 | INFO | [MA-SPI] Stage 3/5 - policy updates 10/50 (current batch 3000) [A0_Policy_loss:0.0041, A1_Policy_loss:0.0031]
2025-10-23 13:27:34,556 | INFO | [MA-SPI] Stage 3/5 - policy updates 20/50 (current batch 3000) [A0_Policy_loss:0.0018, A1_Policy_loss:0.0012]
2025-10-23 13:27:34,599 | INFO | [MA-SPI] Stage 3/5 - policy updates 30/50 (current batch 3000) [A0_Policy_loss:0.0008, A1_Policy_loss:0.0004]
2025-10-23 13:27:34,640 | INFO | [MA-SPI] Stage 3/5 - policy updates 40/50 (current batch 3000) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0002]
2025-10-23 13:27:34,680 | INFO | [MA-SPI] Stage 3/5 - policy updates 50/50 (current batch 3000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0001]
2025-10-23 13:27:34,680 | INFO | [MA-SPI] Stage 3/5 - completed
2025-10-23 13:27:34,680 | INFO | [MA-SPI] Stage 4/5 - collecting trajectories...
2025-10-23 13:27:35,474 | INFO | [MA-SPI] Stage 4/5 - collected 6/30 episodes
2025-10-23 13:27:36,286 | INFO | [MA-SPI] Stage 4/5 - collected 12/30 episodes
2025-10-23 13:27:37,083 | INFO | [MA-SPI] Stage 4/5 - collected 18/30 episodes
2025-10-23 13:27:37,924 | INFO | [MA-SPI] Stage 4/5 - collected 24/30 episodes
2025-10-23 13:27:38,787 | INFO | [MA-SPI] Stage 4/5 - collected 30/30 episodes
2025-10-23 13:27:38,790 | INFO | [MA-SPI] Stage 4/5 - Agent 0 avg episode reward: 109.200 (std 21.564)
2025-10-23 13:27:38,790 | INFO | [MA-SPI] Stage 4/5 - Agent 1 avg episode reward: 106.567 (std 19.626)
2025-10-23 13:27:38,791 | INFO | [MA-SPI] Stage 4/5 - dataset size: 3000 transitions
2025-10-23 13:27:38,892 | INFO | [MA-SPI] Stage 4/5 - Q updates 20/100 (current batch 3000) [A0_Q_loss:0.5164, A1_Q_loss:0.5827]
2025-10-23 13:27:38,997 | INFO | [MA-SPI] Stage 4/5 - Q updates 40/100 (current batch 3000) [A0_Q_loss:0.5028, A1_Q_loss:0.5341]
2025-10-23 13:27:39,100 | INFO | [MA-SPI] Stage 4/5 - Q updates 60/100 (current batch 3000) [A0_Q_loss:0.5019, A1_Q_loss:0.5360]
2025-10-23 13:27:39,193 | INFO | [MA-SPI] Stage 4/5 - Q updates 80/100 (current batch 3000) [A0_Q_loss:0.4935, A1_Q_loss:0.5250]
2025-10-23 13:27:39,280 | INFO | [MA-SPI] Stage 4/5 - Q updates 100/100 (current batch 3000) [A0_Q_loss:0.4896, A1_Q_loss:0.5301]
2025-10-23 13:27:39,320 | INFO | [MA-SPI] Stage 4/5 - policy updates 10/50 (current batch 3000) [A0_Policy_loss:0.0027, A1_Policy_loss:0.0033]
2025-10-23 13:27:39,357 | INFO | [MA-SPI] Stage 4/5 - policy updates 20/50 (current batch 3000) [A0_Policy_loss:0.0010, A1_Policy_loss:0.0011]
2025-10-23 13:27:39,393 | INFO | [MA-SPI] Stage 4/5 - policy updates 30/50 (current batch 3000) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0003]
2025-10-23 13:27:39,434 | INFO | [MA-SPI] Stage 4/5 - policy updates 40/50 (current batch 3000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0002]
2025-10-23 13:27:39,475 | INFO | [MA-SPI] Stage 4/5 - policy updates 50/50 (current batch 3000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0001]
2025-10-23 13:27:39,475 | INFO | [MA-SPI] Stage 4/5 - completed
2025-10-23 13:27:39,476 | INFO | [MA-SPI] Stage 5/5 - collecting trajectories...
2025-10-23 13:27:40,274 | INFO | [MA-SPI] Stage 5/5 - collected 6/30 episodes
2025-10-23 13:27:41,101 | INFO | [MA-SPI] Stage 5/5 - collected 12/30 episodes
2025-10-23 13:27:41,885 | INFO | [MA-SPI] Stage 5/5 - collected 18/30 episodes
2025-10-23 13:27:42,729 | INFO | [MA-SPI] Stage 5/5 - collected 24/30 episodes
2025-10-23 13:27:43,606 | INFO | [MA-SPI] Stage 5/5 - collected 30/30 episodes
2025-10-23 13:27:43,608 | INFO | [MA-SPI] Stage 5/5 - Agent 0 avg episode reward: 109.267 (std 17.497)
2025-10-23 13:27:43,608 | INFO | [MA-SPI] Stage 5/5 - Agent 1 avg episode reward: 111.200 (std 18.251)
2025-10-23 13:27:43,608 | INFO | [MA-SPI] Stage 5/5 - dataset size: 3000 transitions
2025-10-23 13:27:43,714 | INFO | [MA-SPI] Stage 5/5 - Q updates 20/100 (current batch 3000) [A0_Q_loss:0.4865, A1_Q_loss:0.4253]
2025-10-23 13:27:43,812 | INFO | [MA-SPI] Stage 5/5 - Q updates 40/100 (current batch 3000) [A0_Q_loss:0.4779, A1_Q_loss:0.4123]
2025-10-23 13:27:43,902 | INFO | [MA-SPI] Stage 5/5 - Q updates 60/100 (current batch 3000) [A0_Q_loss:0.4571, A1_Q_loss:0.3895]
2025-10-23 13:27:43,991 | INFO | [MA-SPI] Stage 5/5 - Q updates 80/100 (current batch 3000) [A0_Q_loss:0.4505, A1_Q_loss:0.3861]
2025-10-23 13:27:44,110 | INFO | [MA-SPI] Stage 5/5 - Q updates 100/100 (current batch 3000) [A0_Q_loss:0.4563, A1_Q_loss:0.4015]
2025-10-23 13:27:44,145 | INFO | [MA-SPI] Stage 5/5 - policy updates 10/50 (current batch 3000) [A0_Policy_loss:0.0026, A1_Policy_loss:0.0029]
2025-10-23 13:27:44,183 | INFO | [MA-SPI] Stage 5/5 - policy updates 20/50 (current batch 3000) [A0_Policy_loss:0.0011, A1_Policy_loss:0.0011]
2025-10-23 13:27:44,220 | INFO | [MA-SPI] Stage 5/5 - policy updates 30/50 (current batch 3000) [A0_Policy_loss:0.0005, A1_Policy_loss:0.0004]
2025-10-23 13:27:44,257 | INFO | [MA-SPI] Stage 5/5 - policy updates 40/50 (current batch 3000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0002]
2025-10-23 13:27:44,295 | INFO | [MA-SPI] Stage 5/5 - policy updates 50/50 (current batch 3000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0001]
2025-10-23 13:27:44,295 | INFO | [MA-SPI] Stage 5/5 - completed
2025-10-23 13:27:44,367 | INFO | [Experiment:heterogeneous] MA-SPI completed. Artifacts saved to outputs\heterogeneous\ma_spi
2025-10-23 13:27:44,368 | INFO | [Experiment:heterogeneous] Starting MA-LfL...
2025-10-23 13:27:44,368 | INFO | [MA-LfL] Estimating policies...
2025-10-23 13:27:44,368 | INFO | [MA-LfL][Policy] Stage 1/5 - preparing data
2025-10-23 13:27:44,407 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 0: 10/10 epochs completed (loss=1.1227, baseline=1.6094)
2025-10-23 13:27:44,428 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 1: 10/10 epochs completed (loss=1.1188, baseline=1.6094)
2025-10-23 13:27:44,429 | INFO | [MA-LfL][Policy] Stage 2/5 - preparing data
2025-10-23 13:27:44,455 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 0: 10/10 epochs completed (loss=1.0337, baseline=1.6094)
2025-10-23 13:27:44,480 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 1: 10/10 epochs completed (loss=1.0383, baseline=1.6094)
2025-10-23 13:27:44,481 | INFO | [MA-LfL][Policy] Stage 3/5 - preparing data
2025-10-23 13:27:44,504 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 0: 10/10 epochs completed (loss=1.0249, baseline=1.6094)
2025-10-23 13:27:44,527 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 1: 10/10 epochs completed (loss=1.0304, baseline=1.6094)
2025-10-23 13:27:44,529 | INFO | [MA-LfL][Policy] Stage 4/5 - preparing data
2025-10-23 13:27:44,552 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 0: 10/10 epochs completed (loss=1.0741, baseline=1.6094)
2025-10-23 13:27:44,578 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 1: 10/10 epochs completed (loss=1.0651, baseline=1.6094)
2025-10-23 13:27:44,579 | INFO | [MA-LfL][Policy] Stage 5/5 - preparing data
2025-10-23 13:27:44,603 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 0: 10/10 epochs completed (loss=1.0274, baseline=1.6094)
2025-10-23 13:27:44,629 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 1: 10/10 epochs completed (loss=1.0784, baseline=1.6094)
2025-10-23 13:27:44,630 | INFO | [MA-LfL] Computing reward targets...
2025-10-23 13:27:44,630 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 0
2025-10-23 13:27:44,943 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 1
2025-10-23 13:27:45,267 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 0
2025-10-23 13:27:45,585 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 1
2025-10-23 13:27:45,899 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 0
2025-10-23 13:27:46,254 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 1
2025-10-23 13:27:46,598 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 0
2025-10-23 13:27:47,014 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 1
2025-10-23 13:27:47,384 | INFO | [MA-LfL] Learning rewards...
2025-10-23 13:27:47,397 | INFO | [MA-LfL][Reward] Agent 0 - dataset size=12000 (batch=8192, epochs=10000)
2025-10-23 13:28:10,299 | INFO | [MA-LfL][Reward] Agent 0 - 1000/10000 epochs completed (latest loss=0.4982)
2025-10-23 13:28:37,935 | INFO | [MA-LfL][Reward] Agent 0 - 2000/10000 epochs completed (latest loss=0.4877)
2025-10-23 13:29:09,349 | INFO | [MA-LfL][Reward] Agent 0 - 3000/10000 epochs completed (latest loss=0.4875)
2025-10-23 13:29:36,633 | INFO | [MA-LfL][Reward] Agent 0 - 4000/10000 epochs completed (latest loss=0.4896)
2025-10-23 13:30:04,069 | INFO | [MA-LfL][Reward] Agent 0 - 5000/10000 epochs completed (latest loss=0.4868)
2025-10-23 13:30:29,418 | INFO | [MA-LfL][Reward] Agent 0 - 6000/10000 epochs completed (latest loss=0.4924)
2025-10-23 13:30:53,047 | INFO | [MA-LfL][Reward] Agent 0 - 7000/10000 epochs completed (latest loss=0.4875)
2025-10-23 13:31:17,204 | INFO | [MA-LfL][Reward] Agent 0 - 8000/10000 epochs completed (latest loss=0.4898)
2025-10-23 13:31:41,023 | INFO | [MA-LfL][Reward] Agent 0 - 9000/10000 epochs completed (latest loss=0.4929)
2025-10-23 13:32:05,482 | INFO | [MA-LfL][Reward] Agent 0 - 10000/10000 epochs completed (latest loss=0.4952)
2025-10-23 13:32:05,493 | INFO | [MA-LfL][Reward] Agent 1 - dataset size=12000 (batch=8192, epochs=10000)
2025-10-23 13:32:29,089 | INFO | [MA-LfL][Reward] Agent 1 - 1000/10000 epochs completed (latest loss=0.5470)
2025-10-23 13:32:49,533 | INFO | [MA-LfL][Reward] Agent 1 - 2000/10000 epochs completed (latest loss=0.5388)
2025-10-23 13:33:10,032 | INFO | [MA-LfL][Reward] Agent 1 - 3000/10000 epochs completed (latest loss=0.5479)
2025-10-23 13:33:31,034 | INFO | [MA-LfL][Reward] Agent 1 - 4000/10000 epochs completed (latest loss=0.5528)
2025-10-23 13:34:06,242 | INFO | [MA-LfL][Reward] Agent 1 - 5000/10000 epochs completed (latest loss=0.5476)
2025-10-23 13:34:36,100 | INFO | [MA-LfL][Reward] Agent 1 - 6000/10000 epochs completed (latest loss=0.5352)
2025-10-23 13:34:58,445 | INFO | [MA-LfL][Reward] Agent 1 - 7000/10000 epochs completed (latest loss=0.5357)
2025-10-23 13:35:21,205 | INFO | [MA-LfL][Reward] Agent 1 - 8000/10000 epochs completed (latest loss=0.5500)
2025-10-23 13:35:43,056 | INFO | [MA-LfL][Reward] Agent 1 - 9000/10000 epochs completed (latest loss=0.5471)
2025-10-23 13:36:04,233 | INFO | [MA-LfL][Reward] Agent 1 - 10000/10000 epochs completed (latest loss=0.5404)
2025-10-23 13:36:04,235 | INFO | [MA-LfL] Evaluating rewards...
2025-10-23 13:36:05,013 | INFO | [MA-LfL] Pipeline complete.
2025-10-23 13:36:05,013 | INFO | [Experiment:heterogeneous] MA-LfL completed. Artifacts saved to outputs\heterogeneous\ma_lfl
2025-10-23 13:36:05,014 | INFO | [Experiment:heterogeneous] Agent 0 metrics - Pearson: -0.0868 | Spearman: -0.0968
2025-10-23 13:36:05,014 | INFO | [Experiment:heterogeneous] Agent 1 metrics - Pearson: -0.0435 | Spearman: -0.0463
2025-10-23 13:36:05,014 | INFO | [Experiment:heterogeneous] Trend stages: 1, 2, 3, 4
2025-10-23 13:36:05,015 | INFO | [Experiment:heterogeneous] Trend Pearson: -0.3006, -0.2823, -0.2156, -0.1397
2025-10-23 13:36:05,015 | INFO | [Experiment:heterogeneous] Trend Spearman: -0.2021, -0.2499, -0.2505, -0.1867
2025-10-23 13:36:05,015 | INFO | [Experiment:heterogeneous] Agent 0 correlation >=0.4 ? NO
2025-10-23 13:36:05,015 | INFO | [Experiment:heterogeneous] Agent 1 correlation >=0.4 ? NO
2025-10-23 13:36:05,016 | INFO | [Experiment:heterogeneous] Correlation trend non-decreasing? NO
2025-10-23 13:36:05,017 | INFO | Summary saved to outputs\summary.json
2025-10-23 13:36:05,017 | INFO | Run complete.
