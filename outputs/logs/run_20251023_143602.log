2025-10-23 14:36:02,413 | INFO | Logging initialized. Log file: outputs\logs\run_20251023_143602.log
2025-10-23 14:36:02,413 | INFO | Configuration file: config.yaml
2025-10-23 14:36:02,413 | INFO | Reward families to process: heterogeneous
2025-10-23 14:36:02,414 | INFO | MA-SPI iterations=5, episodes/stage=30, episode_length=100
2025-10-23 14:36:02,414 | INFO | MA-LfL reward epochs=10000, reward batch size=8192
2025-10-23 14:36:02,414 | INFO | Log file located at outputs\logs\run_20251023_143602.log
2025-10-23 14:36:02,414 | INFO | Running experiment for reward family: heterogeneous
2025-10-23 14:36:02,414 | INFO | [Experiment:heterogeneous] Starting MA-SPI...
2025-10-23 14:36:03,356 | INFO | [MA-SPI] Starting run with 5 stages, 30 episodes/stage, episode length 100
2025-10-23 14:36:03,368 | INFO | [MA-SPI] Stage 1/5 - collecting trajectories...
2025-10-23 14:36:04,391 | INFO | [MA-SPI] Stage 1/5 - collected 6/30 episodes
2025-10-23 14:36:05,169 | INFO | [MA-SPI] Stage 1/5 - collected 12/30 episodes
2025-10-23 14:36:05,968 | INFO | [MA-SPI] Stage 1/5 - collected 18/30 episodes
2025-10-23 14:36:06,824 | INFO | [MA-SPI] Stage 1/5 - collected 24/30 episodes
2025-10-23 14:36:07,621 | INFO | [MA-SPI] Stage 1/5 - collected 30/30 episodes
2025-10-23 14:36:07,623 | INFO | [MA-SPI] Stage 1/5 - Agent 0 avg episode reward: -133.567 (std 31.196)
2025-10-23 14:36:07,623 | INFO | [MA-SPI] Stage 1/5 - Agent 1 avg episode reward: -113.300 (std 31.693)
2025-10-23 14:36:07,623 | INFO | [MA-SPI] Stage 1/5 - dataset size: 3000 transitions
2025-10-23 14:36:07,915 | INFO | [MA-SPI] Stage 1/5 - Q updates 20/100 (current batch 3000) [A0_Q_loss:3.0577, A1_Q_loss:3.0181]
2025-10-23 14:36:08,010 | INFO | [MA-SPI] Stage 1/5 - Q updates 40/100 (current batch 3000) [A0_Q_loss:1.5579, A1_Q_loss:1.4081]
2025-10-23 14:36:08,101 | INFO | [MA-SPI] Stage 1/5 - Q updates 60/100 (current batch 3000) [A0_Q_loss:1.1305, A1_Q_loss:0.7885]
2025-10-23 14:36:08,193 | INFO | [MA-SPI] Stage 1/5 - Q updates 80/100 (current batch 3000) [A0_Q_loss:0.7832, A1_Q_loss:0.6657]
2025-10-23 14:36:08,274 | INFO | [MA-SPI] Stage 1/5 - Q updates 100/100 (current batch 3000) [A0_Q_loss:0.6631, A1_Q_loss:0.6067]
2025-10-23 14:36:08,323 | INFO | [MA-SPI] Stage 1/5 - policy updates 10/50 (current batch 3000) [A0_Policy_loss:0.0246, A1_Policy_loss:0.0224]
2025-10-23 14:36:08,366 | INFO | [MA-SPI] Stage 1/5 - policy updates 20/50 (current batch 3000) [A0_Policy_loss:0.0081, A1_Policy_loss:0.0053]
2025-10-23 14:36:08,400 | INFO | [MA-SPI] Stage 1/5 - policy updates 30/50 (current batch 3000) [A0_Policy_loss:0.0029, A1_Policy_loss:0.0016]
2025-10-23 14:36:08,434 | INFO | [MA-SPI] Stage 1/5 - policy updates 40/50 (current batch 3000) [A0_Policy_loss:0.0008, A1_Policy_loss:0.0006]
2025-10-23 14:36:08,471 | INFO | [MA-SPI] Stage 1/5 - policy updates 50/50 (current batch 3000) [A0_Policy_loss:0.0003, A1_Policy_loss:0.0002]
2025-10-23 14:36:08,472 | INFO | [MA-SPI] Stage 1/5 - completed
2025-10-23 14:36:08,472 | INFO | [MA-SPI] Stage 2/5 - collecting trajectories...
2025-10-23 14:36:09,277 | INFO | [MA-SPI] Stage 2/5 - collected 6/30 episodes
2025-10-23 14:36:10,084 | INFO | [MA-SPI] Stage 2/5 - collected 12/30 episodes
2025-10-23 14:36:10,970 | INFO | [MA-SPI] Stage 2/5 - collected 18/30 episodes
2025-10-23 14:36:11,856 | INFO | [MA-SPI] Stage 2/5 - collected 24/30 episodes
2025-10-23 14:36:12,758 | INFO | [MA-SPI] Stage 2/5 - collected 30/30 episodes
2025-10-23 14:36:12,760 | INFO | [MA-SPI] Stage 2/5 - Agent 0 avg episode reward: 54.900 (std 37.226)
2025-10-23 14:36:12,761 | INFO | [MA-SPI] Stage 2/5 - Agent 1 avg episode reward: 59.633 (std 35.896)
2025-10-23 14:36:12,761 | INFO | [MA-SPI] Stage 2/5 - dataset size: 3000 transitions
2025-10-23 14:36:12,906 | INFO | [MA-SPI] Stage 2/5 - Q updates 20/100 (current batch 3000) [A0_Q_loss:1.7503, A1_Q_loss:1.1955]
2025-10-23 14:36:13,010 | INFO | [MA-SPI] Stage 2/5 - Q updates 40/100 (current batch 3000) [A0_Q_loss:0.8051, A1_Q_loss:1.0462]
2025-10-23 14:36:13,118 | INFO | [MA-SPI] Stage 2/5 - Q updates 60/100 (current batch 3000) [A0_Q_loss:0.7919, A1_Q_loss:0.9814]
2025-10-23 14:36:13,215 | INFO | [MA-SPI] Stage 2/5 - Q updates 80/100 (current batch 3000) [A0_Q_loss:0.7450, A1_Q_loss:0.9524]
2025-10-23 14:36:13,305 | INFO | [MA-SPI] Stage 2/5 - Q updates 100/100 (current batch 3000) [A0_Q_loss:0.7436, A1_Q_loss:0.9499]
2025-10-23 14:36:13,346 | INFO | [MA-SPI] Stage 2/5 - policy updates 10/50 (current batch 3000) [A0_Policy_loss:0.0128, A1_Policy_loss:0.0057]
2025-10-23 14:36:13,387 | INFO | [MA-SPI] Stage 2/5 - policy updates 20/50 (current batch 3000) [A0_Policy_loss:0.0034, A1_Policy_loss:0.0028]
2025-10-23 14:36:13,425 | INFO | [MA-SPI] Stage 2/5 - policy updates 30/50 (current batch 3000) [A0_Policy_loss:0.0014, A1_Policy_loss:0.0008]
2025-10-23 14:36:13,466 | INFO | [MA-SPI] Stage 2/5 - policy updates 40/50 (current batch 3000) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0003]
2025-10-23 14:36:13,503 | INFO | [MA-SPI] Stage 2/5 - policy updates 50/50 (current batch 3000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0001]
2025-10-23 14:36:13,503 | INFO | [MA-SPI] Stage 2/5 - completed
2025-10-23 14:36:13,504 | INFO | [MA-SPI] Stage 3/5 - collecting trajectories...
2025-10-23 14:36:14,317 | INFO | [MA-SPI] Stage 3/5 - collected 6/30 episodes
2025-10-23 14:36:15,083 | INFO | [MA-SPI] Stage 3/5 - collected 12/30 episodes
2025-10-23 14:36:15,895 | INFO | [MA-SPI] Stage 3/5 - collected 18/30 episodes
2025-10-23 14:36:16,708 | INFO | [MA-SPI] Stage 3/5 - collected 24/30 episodes
2025-10-23 14:36:17,523 | INFO | [MA-SPI] Stage 3/5 - collected 30/30 episodes
2025-10-23 14:36:17,525 | INFO | [MA-SPI] Stage 3/5 - Agent 0 avg episode reward: 110.600 (std 18.804)
2025-10-23 14:36:17,526 | INFO | [MA-SPI] Stage 3/5 - Agent 1 avg episode reward: 106.733 (std 20.258)
2025-10-23 14:36:17,526 | INFO | [MA-SPI] Stage 3/5 - dataset size: 3000 transitions
2025-10-23 14:36:17,655 | INFO | [MA-SPI] Stage 3/5 - Q updates 20/100 (current batch 3000) [A0_Q_loss:0.6757, A1_Q_loss:0.5499]
2025-10-23 14:36:17,749 | INFO | [MA-SPI] Stage 3/5 - Q updates 40/100 (current batch 3000) [A0_Q_loss:0.6097, A1_Q_loss:0.3913]
2025-10-23 14:36:17,840 | INFO | [MA-SPI] Stage 3/5 - Q updates 60/100 (current batch 3000) [A0_Q_loss:0.6006, A1_Q_loss:0.3474]
2025-10-23 14:36:17,980 | INFO | [MA-SPI] Stage 3/5 - Q updates 80/100 (current batch 3000) [A0_Q_loss:0.5920, A1_Q_loss:0.3565]
2025-10-23 14:36:18,066 | INFO | [MA-SPI] Stage 3/5 - Q updates 100/100 (current batch 3000) [A0_Q_loss:0.6041, A1_Q_loss:0.3335]
2025-10-23 14:36:18,102 | INFO | [MA-SPI] Stage 3/5 - policy updates 10/50 (current batch 3000) [A0_Policy_loss:0.0041, A1_Policy_loss:0.0031]
2025-10-23 14:36:18,137 | INFO | [MA-SPI] Stage 3/5 - policy updates 20/50 (current batch 3000) [A0_Policy_loss:0.0018, A1_Policy_loss:0.0012]
2025-10-23 14:36:18,172 | INFO | [MA-SPI] Stage 3/5 - policy updates 30/50 (current batch 3000) [A0_Policy_loss:0.0008, A1_Policy_loss:0.0004]
2025-10-23 14:36:18,210 | INFO | [MA-SPI] Stage 3/5 - policy updates 40/50 (current batch 3000) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0002]
2025-10-23 14:36:18,248 | INFO | [MA-SPI] Stage 3/5 - policy updates 50/50 (current batch 3000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0001]
2025-10-23 14:36:18,249 | INFO | [MA-SPI] Stage 3/5 - completed
2025-10-23 14:36:18,249 | INFO | [MA-SPI] Stage 4/5 - collecting trajectories...
2025-10-23 14:36:19,123 | INFO | [MA-SPI] Stage 4/5 - collected 6/30 episodes
2025-10-23 14:36:19,892 | INFO | [MA-SPI] Stage 4/5 - collected 12/30 episodes
2025-10-23 14:36:20,749 | INFO | [MA-SPI] Stage 4/5 - collected 18/30 episodes
2025-10-23 14:36:21,728 | INFO | [MA-SPI] Stage 4/5 - collected 24/30 episodes
2025-10-23 14:36:22,711 | INFO | [MA-SPI] Stage 4/5 - collected 30/30 episodes
2025-10-23 14:36:22,714 | INFO | [MA-SPI] Stage 4/5 - Agent 0 avg episode reward: 109.200 (std 21.564)
2025-10-23 14:36:22,714 | INFO | [MA-SPI] Stage 4/5 - Agent 1 avg episode reward: 106.567 (std 19.626)
2025-10-23 14:36:22,715 | INFO | [MA-SPI] Stage 4/5 - dataset size: 3000 transitions
2025-10-23 14:36:22,810 | INFO | [MA-SPI] Stage 4/5 - Q updates 20/100 (current batch 3000) [A0_Q_loss:0.5164, A1_Q_loss:0.5827]
2025-10-23 14:36:22,915 | INFO | [MA-SPI] Stage 4/5 - Q updates 40/100 (current batch 3000) [A0_Q_loss:0.5028, A1_Q_loss:0.5341]
2025-10-23 14:36:23,006 | INFO | [MA-SPI] Stage 4/5 - Q updates 60/100 (current batch 3000) [A0_Q_loss:0.5019, A1_Q_loss:0.5360]
2025-10-23 14:36:23,096 | INFO | [MA-SPI] Stage 4/5 - Q updates 80/100 (current batch 3000) [A0_Q_loss:0.4935, A1_Q_loss:0.5250]
2025-10-23 14:36:23,187 | INFO | [MA-SPI] Stage 4/5 - Q updates 100/100 (current batch 3000) [A0_Q_loss:0.4896, A1_Q_loss:0.5301]
2025-10-23 14:36:23,224 | INFO | [MA-SPI] Stage 4/5 - policy updates 10/50 (current batch 3000) [A0_Policy_loss:0.0027, A1_Policy_loss:0.0033]
2025-10-23 14:36:23,266 | INFO | [MA-SPI] Stage 4/5 - policy updates 20/50 (current batch 3000) [A0_Policy_loss:0.0010, A1_Policy_loss:0.0011]
2025-10-23 14:36:23,307 | INFO | [MA-SPI] Stage 4/5 - policy updates 30/50 (current batch 3000) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0003]
2025-10-23 14:36:23,346 | INFO | [MA-SPI] Stage 4/5 - policy updates 40/50 (current batch 3000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0002]
2025-10-23 14:36:23,385 | INFO | [MA-SPI] Stage 4/5 - policy updates 50/50 (current batch 3000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0001]
2025-10-23 14:36:23,385 | INFO | [MA-SPI] Stage 4/5 - completed
2025-10-23 14:36:23,387 | INFO | [MA-SPI] Stage 5/5 - collecting trajectories...
2025-10-23 14:36:24,285 | INFO | [MA-SPI] Stage 5/5 - collected 6/30 episodes
2025-10-23 14:36:25,218 | INFO | [MA-SPI] Stage 5/5 - collected 12/30 episodes
2025-10-23 14:36:26,042 | INFO | [MA-SPI] Stage 5/5 - collected 18/30 episodes
2025-10-23 14:36:27,032 | INFO | [MA-SPI] Stage 5/5 - collected 24/30 episodes
2025-10-23 14:36:27,953 | INFO | [MA-SPI] Stage 5/5 - collected 30/30 episodes
2025-10-23 14:36:27,955 | INFO | [MA-SPI] Stage 5/5 - Agent 0 avg episode reward: 109.267 (std 17.497)
2025-10-23 14:36:27,956 | INFO | [MA-SPI] Stage 5/5 - Agent 1 avg episode reward: 111.200 (std 18.251)
2025-10-23 14:36:27,956 | INFO | [MA-SPI] Stage 5/5 - dataset size: 3000 transitions
2025-10-23 14:36:28,081 | INFO | [MA-SPI] Stage 5/5 - Q updates 20/100 (current batch 3000) [A0_Q_loss:0.4865, A1_Q_loss:0.4253]
2025-10-23 14:36:28,276 | INFO | [MA-SPI] Stage 5/5 - Q updates 40/100 (current batch 3000) [A0_Q_loss:0.4779, A1_Q_loss:0.4123]
2025-10-23 14:36:28,433 | INFO | [MA-SPI] Stage 5/5 - Q updates 60/100 (current batch 3000) [A0_Q_loss:0.4571, A1_Q_loss:0.3895]
2025-10-23 14:36:28,604 | INFO | [MA-SPI] Stage 5/5 - Q updates 80/100 (current batch 3000) [A0_Q_loss:0.4505, A1_Q_loss:0.3861]
2025-10-23 14:36:28,746 | INFO | [MA-SPI] Stage 5/5 - Q updates 100/100 (current batch 3000) [A0_Q_loss:0.4563, A1_Q_loss:0.4015]
2025-10-23 14:36:28,818 | INFO | [MA-SPI] Stage 5/5 - policy updates 10/50 (current batch 3000) [A0_Policy_loss:0.0026, A1_Policy_loss:0.0029]
2025-10-23 14:36:28,866 | INFO | [MA-SPI] Stage 5/5 - policy updates 20/50 (current batch 3000) [A0_Policy_loss:0.0011, A1_Policy_loss:0.0011]
2025-10-23 14:36:28,944 | INFO | [MA-SPI] Stage 5/5 - policy updates 30/50 (current batch 3000) [A0_Policy_loss:0.0005, A1_Policy_loss:0.0004]
2025-10-23 14:36:29,040 | INFO | [MA-SPI] Stage 5/5 - policy updates 40/50 (current batch 3000) [A0_Policy_loss:0.0002, A1_Policy_loss:0.0002]
2025-10-23 14:36:29,112 | INFO | [MA-SPI] Stage 5/5 - policy updates 50/50 (current batch 3000) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0001]
2025-10-23 14:36:29,112 | INFO | [MA-SPI] Stage 5/5 - completed
2025-10-23 14:36:29,186 | INFO | [Experiment:heterogeneous] MA-SPI completed. Artifacts saved to outputs\heterogeneous\ma_spi
2025-10-23 14:36:29,187 | INFO | [Experiment:heterogeneous] Starting MA-LfL...
2025-10-23 14:36:29,187 | INFO | [MA-LfL] Estimating policies...
2025-10-23 14:36:29,187 | INFO | [MA-LfL][Policy] Stage 1/5 - preparing data
2025-10-23 14:36:29,246 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 0: 10/10 epochs completed (loss=1.1227, baseline=1.6094)
2025-10-23 14:36:29,288 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 1: 10/10 epochs completed (loss=1.1188, baseline=1.6094)
2025-10-23 14:36:29,289 | INFO | [MA-LfL][Policy] Stage 2/5 - preparing data
2025-10-23 14:36:29,342 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 0: 10/10 epochs completed (loss=1.0337, baseline=1.6094)
2025-10-23 14:36:29,393 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 1: 10/10 epochs completed (loss=1.0383, baseline=1.6094)
2025-10-23 14:36:29,394 | INFO | [MA-LfL][Policy] Stage 3/5 - preparing data
2025-10-23 14:36:29,447 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 0: 10/10 epochs completed (loss=1.0249, baseline=1.6094)
2025-10-23 14:36:29,505 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 1: 10/10 epochs completed (loss=1.0304, baseline=1.6094)
2025-10-23 14:36:29,506 | INFO | [MA-LfL][Policy] Stage 4/5 - preparing data
2025-10-23 14:36:29,563 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 0: 10/10 epochs completed (loss=1.0741, baseline=1.6094)
2025-10-23 14:36:29,613 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 1: 10/10 epochs completed (loss=1.0651, baseline=1.6094)
2025-10-23 14:36:29,613 | INFO | [MA-LfL][Policy] Stage 5/5 - preparing data
2025-10-23 14:36:29,661 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 0: 10/10 epochs completed (loss=1.0274, baseline=1.6094)
2025-10-23 14:36:29,689 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 1: 10/10 epochs completed (loss=1.0784, baseline=1.6094)
2025-10-23 14:36:29,690 | INFO | [MA-LfL] Computing reward targets...
2025-10-23 14:36:29,690 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 0
2025-10-23 14:36:30,087 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 1
2025-10-23 14:36:30,508 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 0
2025-10-23 14:36:30,938 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 1
2025-10-23 14:36:31,437 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 0
2025-10-23 14:36:31,891 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 1
2025-10-23 14:36:32,352 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 0
2025-10-23 14:36:32,852 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 1
2025-10-23 14:36:33,321 | INFO | [MA-LfL] Learning rewards...
2025-10-23 14:36:33,338 | INFO | [MA-LfL][Reward] Agent 0 - dataset size=12000 (batch=8192, epochs=10000)
2025-10-23 14:37:00,701 | INFO | [MA-LfL][Reward] Agent 0 - 1000/10000 epochs completed (latest loss=0.4976)
2025-10-23 14:37:24,396 | INFO | [MA-LfL][Reward] Agent 0 - 2000/10000 epochs completed (latest loss=0.4900)
2025-10-23 14:37:47,012 | INFO | [MA-LfL][Reward] Agent 0 - 3000/10000 epochs completed (latest loss=0.4886)
2025-10-23 14:38:08,928 | INFO | [MA-LfL][Reward] Agent 0 - 4000/10000 epochs completed (latest loss=0.4896)
2025-10-23 14:38:30,068 | INFO | [MA-LfL][Reward] Agent 0 - 5000/10000 epochs completed (latest loss=0.4877)
2025-10-23 14:38:51,335 | INFO | [MA-LfL][Reward] Agent 0 - 6000/10000 epochs completed (latest loss=0.4918)
2025-10-23 14:39:13,069 | INFO | [MA-LfL][Reward] Agent 0 - 7000/10000 epochs completed (latest loss=0.4870)
2025-10-23 14:39:34,532 | INFO | [MA-LfL][Reward] Agent 0 - 8000/10000 epochs completed (latest loss=0.4891)
2025-10-23 14:39:57,221 | INFO | [MA-LfL][Reward] Agent 0 - 9000/10000 epochs completed (latest loss=0.4919)
2025-10-23 14:40:20,061 | INFO | [MA-LfL][Reward] Agent 0 - 10000/10000 epochs completed (latest loss=0.4955)
2025-10-23 14:40:20,072 | INFO | [MA-LfL][Reward] Agent 1 - dataset size=12000 (batch=8192, epochs=10000)
2025-10-23 14:40:47,346 | INFO | [MA-LfL][Reward] Agent 1 - 1000/10000 epochs completed (latest loss=0.5493)
2025-10-23 14:41:11,255 | INFO | [MA-LfL][Reward] Agent 1 - 2000/10000 epochs completed (latest loss=0.5378)
2025-10-23 14:41:34,723 | INFO | [MA-LfL][Reward] Agent 1 - 3000/10000 epochs completed (latest loss=0.5463)
2025-10-23 14:41:59,712 | INFO | [MA-LfL][Reward] Agent 1 - 4000/10000 epochs completed (latest loss=0.5515)
2025-10-23 14:42:23,922 | INFO | [MA-LfL][Reward] Agent 1 - 5000/10000 epochs completed (latest loss=0.5484)
2025-10-23 14:42:47,847 | INFO | [MA-LfL][Reward] Agent 1 - 6000/10000 epochs completed (latest loss=0.5344)
2025-10-23 14:43:12,656 | INFO | [MA-LfL][Reward] Agent 1 - 7000/10000 epochs completed (latest loss=0.5349)
2025-10-23 14:43:35,872 | INFO | [MA-LfL][Reward] Agent 1 - 8000/10000 epochs completed (latest loss=0.5511)
2025-10-23 14:43:58,973 | INFO | [MA-LfL][Reward] Agent 1 - 9000/10000 epochs completed (latest loss=0.5486)
2025-10-23 14:44:21,934 | INFO | [MA-LfL][Reward] Agent 1 - 10000/10000 epochs completed (latest loss=0.5402)
2025-10-23 14:44:21,936 | INFO | [MA-LfL] Evaluating rewards...
2025-10-23 14:44:22,765 | INFO | [MA-LfL] Pipeline complete.
2025-10-23 14:44:22,765 | INFO | [Experiment:heterogeneous] MA-LfL completed. Artifacts saved to outputs\heterogeneous\ma_lfl
2025-10-23 14:44:22,766 | INFO | [Experiment:heterogeneous] Agent 0 metrics - Pearson: 0.1095 | Spearman: 0.1223
2025-10-23 14:44:22,766 | INFO | [Experiment:heterogeneous] Agent 1 metrics - Pearson: 0.0477 | Spearman: 0.0524
2025-10-23 14:44:22,766 | INFO | [Experiment:heterogeneous] Trend stages: 1, 2, 3, 4
2025-10-23 14:44:22,766 | INFO | [Experiment:heterogeneous] Trend Pearson: 0.1568, 0.4028, 0.5521, 0.4040
2025-10-23 14:44:22,766 | INFO | [Experiment:heterogeneous] Trend Spearman: 0.1636, 0.4588, 0.5177, 0.4305
2025-10-23 14:44:22,766 | INFO | [Experiment:heterogeneous] Agent 0 correlation >=0.4 ? NO
2025-10-23 14:44:22,766 | INFO | [Experiment:heterogeneous] Agent 1 correlation >=0.4 ? NO
2025-10-23 14:44:22,766 | INFO | [Experiment:heterogeneous] Correlation trend non-decreasing? NO
2025-10-23 14:44:22,768 | INFO | Summary saved to outputs\summary.json
2025-10-23 14:44:22,768 | INFO | Run complete.
