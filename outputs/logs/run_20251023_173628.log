2025-10-23 17:36:28,242 | INFO | Logging initialized. Log file: outputs\logs\run_20251023_173628.log
2025-10-23 17:36:28,242 | INFO | Configuration file: config.yaml
2025-10-23 17:36:28,242 | INFO | Reward families to process: heterogeneous
2025-10-23 17:36:28,243 | INFO | MA-SPI iterations=5, episodes/stage=15, episode_length=100
2025-10-23 17:36:28,243 | INFO | MA-LfL reward epochs=10000, reward batch size=8192
2025-10-23 17:36:28,243 | INFO | Log file located at outputs\logs\run_20251023_173628.log
2025-10-23 17:36:28,243 | INFO | Running experiment for reward family: heterogeneous
2025-10-23 17:36:28,244 | INFO | [Experiment:heterogeneous] Starting MA-SPI...
2025-10-23 17:36:29,264 | INFO | [MA-SPI] Starting run with 5 stages, 15 episodes/stage, episode length 100
2025-10-23 17:36:29,266 | INFO | [MA-SPI] Stage 1/5 - collecting trajectories...
2025-10-23 17:36:30,057 | INFO | [MA-SPI] Stage 1/5 - collected 3/15 episodes
2025-10-23 17:36:30,623 | INFO | [MA-SPI] Stage 1/5 - collected 6/15 episodes
2025-10-23 17:36:31,167 | INFO | [MA-SPI] Stage 1/5 - collected 9/15 episodes
2025-10-23 17:36:31,775 | INFO | [MA-SPI] Stage 1/5 - collected 12/15 episodes
2025-10-23 17:36:32,331 | INFO | [MA-SPI] Stage 1/5 - collected 15/15 episodes
2025-10-23 17:36:32,332 | INFO | [MA-SPI] Stage 1/5 - Agent 0 avg episode reward: -136.667 (std 28.883)
2025-10-23 17:36:32,332 | INFO | [MA-SPI] Stage 1/5 - Agent 1 avg episode reward: -117.600 (std 32.104)
2025-10-23 17:36:32,334 | INFO | [MA-SPI] Stage 1/5 - dataset size: 1500 transitions
2025-10-23 17:36:32,602 | INFO | [MA-SPI] Stage 1/5 - Q updates 20/100 (current batch 1500) [A0_Q_loss:2.8543, A1_Q_loss:3.0542]
2025-10-23 17:36:32,741 | INFO | [MA-SPI] Stage 1/5 - Q updates 40/100 (current batch 1500) [A0_Q_loss:1.4510, A1_Q_loss:1.3609]
2025-10-23 17:36:32,874 | INFO | [MA-SPI] Stage 1/5 - Q updates 60/100 (current batch 1500) [A0_Q_loss:1.1157, A1_Q_loss:0.9456]
2025-10-23 17:36:32,989 | INFO | [MA-SPI] Stage 1/5 - Q updates 80/100 (current batch 1500) [A0_Q_loss:0.8393, A1_Q_loss:0.6929]
2025-10-23 17:36:33,114 | INFO | [MA-SPI] Stage 1/5 - Q updates 100/100 (current batch 1500) [A0_Q_loss:0.6768, A1_Q_loss:0.5886]
2025-10-23 17:36:33,181 | INFO | [MA-SPI] Stage 1/5 - policy updates 10/50 (current batch 1500) [A0_Policy_loss:0.0233, A1_Policy_loss:0.0197]
2025-10-23 17:36:33,255 | INFO | [MA-SPI] Stage 1/5 - policy updates 20/50 (current batch 1500) [A0_Policy_loss:0.0074, A1_Policy_loss:0.0052]
2025-10-23 17:36:33,304 | INFO | [MA-SPI] Stage 1/5 - policy updates 30/50 (current batch 1500) [A0_Policy_loss:0.0026, A1_Policy_loss:0.0018]
2025-10-23 17:36:33,360 | INFO | [MA-SPI] Stage 1/5 - policy updates 40/50 (current batch 1500) [A0_Policy_loss:0.0008, A1_Policy_loss:0.0006]
2025-10-23 17:36:33,407 | INFO | [MA-SPI] Stage 1/5 - policy updates 50/50 (current batch 1500) [A0_Policy_loss:0.0003, A1_Policy_loss:0.0002]
2025-10-23 17:36:33,408 | INFO | [MA-SPI] Stage 1/5 - completed
2025-10-23 17:36:33,408 | INFO | [MA-SPI] Stage 2/5 - collecting trajectories...
2025-10-23 17:36:33,891 | INFO | [MA-SPI] Stage 2/5 - collected 3/15 episodes
2025-10-23 17:36:34,428 | INFO | [MA-SPI] Stage 2/5 - collected 6/15 episodes
2025-10-23 17:36:34,952 | INFO | [MA-SPI] Stage 2/5 - collected 9/15 episodes
2025-10-23 17:36:35,494 | INFO | [MA-SPI] Stage 2/5 - collected 12/15 episodes
2025-10-23 17:36:36,010 | INFO | [MA-SPI] Stage 2/5 - collected 15/15 episodes
2025-10-23 17:36:36,012 | INFO | [MA-SPI] Stage 2/5 - Agent 0 avg episode reward: 53.067 (std 32.285)
2025-10-23 17:36:36,012 | INFO | [MA-SPI] Stage 2/5 - Agent 1 avg episode reward: 60.000 (std 23.458)
2025-10-23 17:36:36,012 | INFO | [MA-SPI] Stage 2/5 - dataset size: 1500 transitions
2025-10-23 17:36:36,134 | INFO | [MA-SPI] Stage 2/5 - Q updates 20/100 (current batch 1500) [A0_Q_loss:1.7669, A1_Q_loss:1.5880]
2025-10-23 17:36:36,260 | INFO | [MA-SPI] Stage 2/5 - Q updates 40/100 (current batch 1500) [A0_Q_loss:0.8076, A1_Q_loss:1.2151]
2025-10-23 17:36:36,411 | INFO | [MA-SPI] Stage 2/5 - Q updates 60/100 (current batch 1500) [A0_Q_loss:0.7033, A1_Q_loss:1.0164]
2025-10-23 17:36:36,539 | INFO | [MA-SPI] Stage 2/5 - Q updates 80/100 (current batch 1500) [A0_Q_loss:0.6396, A1_Q_loss:1.0301]
2025-10-23 17:36:36,658 | INFO | [MA-SPI] Stage 2/5 - Q updates 100/100 (current batch 1500) [A0_Q_loss:0.6277, A1_Q_loss:0.9989]
2025-10-23 17:36:36,711 | INFO | [MA-SPI] Stage 2/5 - policy updates 10/50 (current batch 1500) [A0_Policy_loss:0.0181, A1_Policy_loss:0.0075]
2025-10-23 17:36:36,765 | INFO | [MA-SPI] Stage 2/5 - policy updates 20/50 (current batch 1500) [A0_Policy_loss:0.0052, A1_Policy_loss:0.0025]
2025-10-23 17:36:36,819 | INFO | [MA-SPI] Stage 2/5 - policy updates 30/50 (current batch 1500) [A0_Policy_loss:0.0019, A1_Policy_loss:0.0008]
2025-10-23 17:36:36,862 | INFO | [MA-SPI] Stage 2/5 - policy updates 40/50 (current batch 1500) [A0_Policy_loss:0.0010, A1_Policy_loss:0.0003]
2025-10-23 17:36:36,914 | INFO | [MA-SPI] Stage 2/5 - policy updates 50/50 (current batch 1500) [A0_Policy_loss:0.0003, A1_Policy_loss:0.0001]
2025-10-23 17:36:36,915 | INFO | [MA-SPI] Stage 2/5 - completed
2025-10-23 17:36:36,915 | INFO | [MA-SPI] Stage 3/5 - collecting trajectories...
2025-10-23 17:36:37,387 | INFO | [MA-SPI] Stage 3/5 - collected 3/15 episodes
2025-10-23 17:36:37,899 | INFO | [MA-SPI] Stage 3/5 - collected 6/15 episodes
2025-10-23 17:36:38,423 | INFO | [MA-SPI] Stage 3/5 - collected 9/15 episodes
2025-10-23 17:36:38,890 | INFO | [MA-SPI] Stage 3/5 - collected 12/15 episodes
2025-10-23 17:36:39,380 | INFO | [MA-SPI] Stage 3/5 - collected 15/15 episodes
2025-10-23 17:36:39,382 | INFO | [MA-SPI] Stage 3/5 - Agent 0 avg episode reward: 106.333 (std 23.815)
2025-10-23 17:36:39,382 | INFO | [MA-SPI] Stage 3/5 - Agent 1 avg episode reward: 114.667 (std 23.601)
2025-10-23 17:36:39,382 | INFO | [MA-SPI] Stage 3/5 - dataset size: 1500 transitions
2025-10-23 17:36:39,494 | INFO | [MA-SPI] Stage 3/5 - Q updates 20/100 (current batch 1500) [A0_Q_loss:0.6447, A1_Q_loss:0.5151]
2025-10-23 17:36:39,623 | INFO | [MA-SPI] Stage 3/5 - Q updates 40/100 (current batch 1500) [A0_Q_loss:0.4769, A1_Q_loss:0.4185]
2025-10-23 17:36:39,734 | INFO | [MA-SPI] Stage 3/5 - Q updates 60/100 (current batch 1500) [A0_Q_loss:0.4680, A1_Q_loss:0.4087]
2025-10-23 17:36:39,837 | INFO | [MA-SPI] Stage 3/5 - Q updates 80/100 (current batch 1500) [A0_Q_loss:0.4754, A1_Q_loss:0.3814]
2025-10-23 17:36:39,932 | INFO | [MA-SPI] Stage 3/5 - Q updates 100/100 (current batch 1500) [A0_Q_loss:0.4551, A1_Q_loss:0.3780]
2025-10-23 17:36:39,973 | INFO | [MA-SPI] Stage 3/5 - policy updates 10/50 (current batch 1500) [A0_Policy_loss:0.0061, A1_Policy_loss:0.0043]
2025-10-23 17:36:40,021 | INFO | [MA-SPI] Stage 3/5 - policy updates 20/50 (current batch 1500) [A0_Policy_loss:0.0018, A1_Policy_loss:0.0014]
2025-10-23 17:36:40,061 | INFO | [MA-SPI] Stage 3/5 - policy updates 30/50 (current batch 1500) [A0_Policy_loss:0.0009, A1_Policy_loss:0.0005]
2025-10-23 17:36:40,106 | INFO | [MA-SPI] Stage 3/5 - policy updates 40/50 (current batch 1500) [A0_Policy_loss:0.0003, A1_Policy_loss:0.0003]
2025-10-23 17:36:40,147 | INFO | [MA-SPI] Stage 3/5 - policy updates 50/50 (current batch 1500) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0001]
2025-10-23 17:36:40,147 | INFO | [MA-SPI] Stage 3/5 - completed
2025-10-23 17:36:40,147 | INFO | [MA-SPI] Stage 4/5 - collecting trajectories...
2025-10-23 17:36:40,608 | INFO | [MA-SPI] Stage 4/5 - collected 3/15 episodes
2025-10-23 17:36:41,077 | INFO | [MA-SPI] Stage 4/5 - collected 6/15 episodes
2025-10-23 17:36:41,524 | INFO | [MA-SPI] Stage 4/5 - collected 9/15 episodes
2025-10-23 17:36:42,005 | INFO | [MA-SPI] Stage 4/5 - collected 12/15 episodes
2025-10-23 17:36:42,468 | INFO | [MA-SPI] Stage 4/5 - collected 15/15 episodes
2025-10-23 17:36:42,470 | INFO | [MA-SPI] Stage 4/5 - Agent 0 avg episode reward: 118.267 (std 17.726)
2025-10-23 17:36:42,470 | INFO | [MA-SPI] Stage 4/5 - Agent 1 avg episode reward: 113.133 (std 17.936)
2025-10-23 17:36:42,470 | INFO | [MA-SPI] Stage 4/5 - dataset size: 1500 transitions
2025-10-23 17:36:42,572 | INFO | [MA-SPI] Stage 4/5 - Q updates 20/100 (current batch 1500) [A0_Q_loss:0.5598, A1_Q_loss:0.4251]
2025-10-23 17:36:42,682 | INFO | [MA-SPI] Stage 4/5 - Q updates 40/100 (current batch 1500) [A0_Q_loss:0.5833, A1_Q_loss:0.3760]
2025-10-23 17:36:42,810 | INFO | [MA-SPI] Stage 4/5 - Q updates 60/100 (current batch 1500) [A0_Q_loss:0.5851, A1_Q_loss:0.4131]
2025-10-23 17:36:42,922 | INFO | [MA-SPI] Stage 4/5 - Q updates 80/100 (current batch 1500) [A0_Q_loss:0.5843, A1_Q_loss:0.3920]
2025-10-23 17:36:43,021 | INFO | [MA-SPI] Stage 4/5 - Q updates 100/100 (current batch 1500) [A0_Q_loss:0.5950, A1_Q_loss:0.3813]
2025-10-23 17:36:43,065 | INFO | [MA-SPI] Stage 4/5 - policy updates 10/50 (current batch 1500) [A0_Policy_loss:0.0037, A1_Policy_loss:0.0029]
2025-10-23 17:36:43,103 | INFO | [MA-SPI] Stage 4/5 - policy updates 20/50 (current batch 1500) [A0_Policy_loss:0.0013, A1_Policy_loss:0.0010]
2025-10-23 17:36:43,143 | INFO | [MA-SPI] Stage 4/5 - policy updates 30/50 (current batch 1500) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0004]
2025-10-23 17:36:43,183 | INFO | [MA-SPI] Stage 4/5 - policy updates 40/50 (current batch 1500) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0002]
2025-10-23 17:36:43,222 | INFO | [MA-SPI] Stage 4/5 - policy updates 50/50 (current batch 1500) [A0_Policy_loss:0.0000, A1_Policy_loss:0.0001]
2025-10-23 17:36:43,223 | INFO | [MA-SPI] Stage 4/5 - completed
2025-10-23 17:36:43,223 | INFO | [MA-SPI] Stage 5/5 - collecting trajectories...
2025-10-23 17:36:43,688 | INFO | [MA-SPI] Stage 5/5 - collected 3/15 episodes
2025-10-23 17:36:44,170 | INFO | [MA-SPI] Stage 5/5 - collected 6/15 episodes
2025-10-23 17:36:44,629 | INFO | [MA-SPI] Stage 5/5 - collected 9/15 episodes
2025-10-23 17:36:45,121 | INFO | [MA-SPI] Stage 5/5 - collected 12/15 episodes
2025-10-23 17:36:45,599 | INFO | [MA-SPI] Stage 5/5 - collected 15/15 episodes
2025-10-23 17:36:45,601 | INFO | [MA-SPI] Stage 5/5 - Agent 0 avg episode reward: 112.400 (std 16.584)
2025-10-23 17:36:45,601 | INFO | [MA-SPI] Stage 5/5 - Agent 1 avg episode reward: 107.933 (std 13.087)
2025-10-23 17:36:45,601 | INFO | [MA-SPI] Stage 5/5 - dataset size: 1500 transitions
2025-10-23 17:36:45,710 | INFO | [MA-SPI] Stage 5/5 - Q updates 20/100 (current batch 1500) [A0_Q_loss:0.5385, A1_Q_loss:0.4213]
2025-10-23 17:36:45,817 | INFO | [MA-SPI] Stage 5/5 - Q updates 40/100 (current batch 1500) [A0_Q_loss:0.5087, A1_Q_loss:0.3812]
2025-10-23 17:36:45,970 | INFO | [MA-SPI] Stage 5/5 - Q updates 60/100 (current batch 1500) [A0_Q_loss:0.5229, A1_Q_loss:0.3716]
2025-10-23 17:36:46,067 | INFO | [MA-SPI] Stage 5/5 - Q updates 80/100 (current batch 1500) [A0_Q_loss:0.5014, A1_Q_loss:0.3674]
2025-10-23 17:36:46,189 | INFO | [MA-SPI] Stage 5/5 - Q updates 100/100 (current batch 1500) [A0_Q_loss:0.5067, A1_Q_loss:0.3629]
2025-10-23 17:36:46,231 | INFO | [MA-SPI] Stage 5/5 - policy updates 10/50 (current batch 1500) [A0_Policy_loss:0.0031, A1_Policy_loss:0.0025]
2025-10-23 17:36:46,276 | INFO | [MA-SPI] Stage 5/5 - policy updates 20/50 (current batch 1500) [A0_Policy_loss:0.0010, A1_Policy_loss:0.0011]
2025-10-23 17:36:46,317 | INFO | [MA-SPI] Stage 5/5 - policy updates 30/50 (current batch 1500) [A0_Policy_loss:0.0004, A1_Policy_loss:0.0005]
2025-10-23 17:36:46,361 | INFO | [MA-SPI] Stage 5/5 - policy updates 40/50 (current batch 1500) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0003]
2025-10-23 17:36:46,411 | INFO | [MA-SPI] Stage 5/5 - policy updates 50/50 (current batch 1500) [A0_Policy_loss:0.0001, A1_Policy_loss:0.0002]
2025-10-23 17:36:46,411 | INFO | [MA-SPI] Stage 5/5 - completed
2025-10-23 17:36:46,457 | INFO | [Experiment:heterogeneous] MA-SPI completed. Artifacts saved to outputs\heterogeneous\ma_spi
2025-10-23 17:36:46,457 | INFO | [Experiment:heterogeneous] Starting MA-LfL...
2025-10-23 17:36:46,457 | INFO | [MA-LfL] Estimating policies...
2025-10-23 17:36:46,457 | INFO | [MA-LfL][Policy] Stage 1/5 - preparing data
2025-10-23 17:36:46,502 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 0: 10/10 epochs completed (loss=1.1186, baseline=1.6094)
2025-10-23 17:36:46,533 | INFO | [MA-LfL][Policy] Stage 1/5 - Agent 1: 10/10 epochs completed (loss=1.0979, baseline=1.6094)
2025-10-23 17:36:46,535 | INFO | [MA-LfL][Policy] Stage 2/5 - preparing data
2025-10-23 17:36:46,562 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 0: 10/10 epochs completed (loss=1.0415, baseline=1.6094)
2025-10-23 17:36:46,596 | INFO | [MA-LfL][Policy] Stage 2/5 - Agent 1: 10/10 epochs completed (loss=1.0158, baseline=1.6094)
2025-10-23 17:36:46,596 | INFO | [MA-LfL][Policy] Stage 3/5 - preparing data
2025-10-23 17:36:46,626 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 0: 10/10 epochs completed (loss=1.0168, baseline=1.6094)
2025-10-23 17:36:46,660 | INFO | [MA-LfL][Policy] Stage 3/5 - Agent 1: 10/10 epochs completed (loss=1.0153, baseline=1.6094)
2025-10-23 17:36:46,661 | INFO | [MA-LfL][Policy] Stage 4/5 - preparing data
2025-10-23 17:36:46,690 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 0: 10/10 epochs completed (loss=0.9902, baseline=1.6094)
2025-10-23 17:36:46,727 | INFO | [MA-LfL][Policy] Stage 4/5 - Agent 1: 10/10 epochs completed (loss=1.0106, baseline=1.6094)
2025-10-23 17:36:46,727 | INFO | [MA-LfL][Policy] Stage 5/5 - preparing data
2025-10-23 17:36:46,757 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 0: 10/10 epochs completed (loss=1.0334, baseline=1.6094)
2025-10-23 17:36:46,790 | INFO | [MA-LfL][Policy] Stage 5/5 - Agent 1: 10/10 epochs completed (loss=1.0042, baseline=1.6094)
2025-10-23 17:36:46,792 | INFO | [MA-LfL] Computing reward targets...
2025-10-23 17:36:46,792 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 0
2025-10-23 17:36:47,203 | INFO | [MA-LfL][Targets] Stage 1/4 - Agent 1
2025-10-23 17:36:47,583 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 0
2025-10-23 17:36:47,970 | INFO | [MA-LfL][Targets] Stage 2/4 - Agent 1
2025-10-23 17:36:48,388 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 0
2025-10-23 17:36:48,793 | INFO | [MA-LfL][Targets] Stage 3/4 - Agent 1
2025-10-23 17:36:49,205 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 0
2025-10-23 17:36:49,604 | INFO | [MA-LfL][Targets] Stage 4/4 - Agent 1
2025-10-23 17:36:50,000 | INFO | [MA-LfL] Learning rewards...
2025-10-23 17:36:50,017 | INFO | [MA-LfL][Reward] Agent 0 - dataset size=6000 (batch=6000, epochs=10000)
2025-10-23 17:37:03,057 | INFO | [MA-LfL][Reward] Agent 0 - 1000/10000 epochs completed (latest loss=0.5327)
2025-10-23 17:37:15,934 | INFO | [MA-LfL][Reward] Agent 0 - 2000/10000 epochs completed (latest loss=0.5323)
2025-10-23 17:37:28,170 | INFO | [MA-LfL][Reward] Agent 0 - 3000/10000 epochs completed (latest loss=0.5322)
2025-10-23 17:37:40,659 | INFO | [MA-LfL][Reward] Agent 0 - 4000/10000 epochs completed (latest loss=0.5326)
2025-10-23 17:37:54,863 | INFO | [MA-LfL][Reward] Agent 0 - 5000/10000 epochs completed (latest loss=0.5323)
2025-10-23 17:38:06,861 | INFO | [MA-LfL][Reward] Agent 0 - 6000/10000 epochs completed (latest loss=0.5322)
2025-10-23 17:38:17,555 | INFO | [MA-LfL][Reward] Agent 0 - 7000/10000 epochs completed (latest loss=0.5322)
2025-10-23 17:38:27,844 | INFO | [MA-LfL][Reward] Agent 0 - 8000/10000 epochs completed (latest loss=0.5328)
2025-10-23 17:38:38,755 | INFO | [MA-LfL][Reward] Agent 0 - 9000/10000 epochs completed (latest loss=0.5322)
2025-10-23 17:38:50,229 | INFO | [MA-LfL][Reward] Agent 0 - 10000/10000 epochs completed (latest loss=0.5322)
2025-10-23 17:38:50,235 | INFO | [MA-LfL][Reward] Agent 1 - dataset size=6000 (batch=6000, epochs=10000)
2025-10-23 17:39:01,752 | INFO | [MA-LfL][Reward] Agent 1 - 1000/10000 epochs completed (latest loss=0.5989)
2025-10-23 17:39:13,311 | INFO | [MA-LfL][Reward] Agent 1 - 2000/10000 epochs completed (latest loss=0.6099)
2025-10-23 17:39:25,431 | INFO | [MA-LfL][Reward] Agent 1 - 3000/10000 epochs completed (latest loss=0.5986)
2025-10-23 17:39:37,881 | INFO | [MA-LfL][Reward] Agent 1 - 4000/10000 epochs completed (latest loss=0.5986)
2025-10-23 17:39:49,567 | INFO | [MA-LfL][Reward] Agent 1 - 5000/10000 epochs completed (latest loss=0.5986)
2025-10-23 17:40:00,280 | INFO | [MA-LfL][Reward] Agent 1 - 6000/10000 epochs completed (latest loss=0.5986)
2025-10-23 17:40:11,691 | INFO | [MA-LfL][Reward] Agent 1 - 7000/10000 epochs completed (latest loss=0.5987)
2025-10-23 17:40:23,630 | INFO | [MA-LfL][Reward] Agent 1 - 8000/10000 epochs completed (latest loss=0.6013)
2025-10-23 17:40:35,122 | INFO | [MA-LfL][Reward] Agent 1 - 9000/10000 epochs completed (latest loss=0.5986)
2025-10-23 17:40:47,544 | INFO | [MA-LfL][Reward] Agent 1 - 10000/10000 epochs completed (latest loss=0.5986)
2025-10-23 17:40:47,544 | INFO | [MA-LfL] Evaluating rewards...
2025-10-23 17:40:48,415 | INFO | [MA-LfL] Pipeline complete.
2025-10-23 17:40:48,415 | INFO | [Experiment:heterogeneous] MA-LfL completed. Artifacts saved to outputs\heterogeneous\ma_lfl
2025-10-23 17:40:48,415 | INFO | [Experiment:heterogeneous] Agent 0 metrics - Pearson: 0.1544 | Spearman: 0.1624
2025-10-23 17:40:48,415 | INFO | [Experiment:heterogeneous] Agent 1 metrics - Pearson: 0.1354 | Spearman: 0.1330
2025-10-23 17:40:48,415 | INFO | [Experiment:heterogeneous] Trend stages: 1, 2, 3, 4
2025-10-23 17:40:48,415 | INFO | [Experiment:heterogeneous] Trend Pearson: 0.4017, 0.5113, 0.6723, 0.6531
2025-10-23 17:40:48,415 | INFO | [Experiment:heterogeneous] Trend Spearman: 0.4030, 0.5967, 0.4801, 0.6223
2025-10-23 17:40:48,415 | INFO | [Experiment:heterogeneous] Agent 0 correlation >=0.4 ? NO
2025-10-23 17:40:48,415 | INFO | [Experiment:heterogeneous] Agent 1 correlation >=0.4 ? NO
2025-10-23 17:40:48,415 | INFO | [Experiment:heterogeneous] Correlation trend non-decreasing? NO
2025-10-23 17:40:48,417 | INFO | Summary saved to outputs\summary.json
2025-10-23 17:40:48,417 | INFO | Run complete.
