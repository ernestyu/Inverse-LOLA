alpha: 3.0
gamma: 0.9
horizon: 10
device: cuda

environment:
  grid_size: 3
  start_positions:
    - [0, 0]
    - [0, 0]
  goal_position: [2, 2]
  reward_family: homogeneous

maspi:
  num_iterations: 5
  evaluation_episodes_per_iteration: 10
  episode_length: 100
  q_updates_per_stage: 100
  policy_updates_per_stage: 50
  seed: 42
  update_batch_size: 16384
  min_update_batch_size: 512
  batch_shrink_factor: 0.5
  batch_growth_factor: 1.3
  cache_stage_on_gpu: true
  pin_memory: true
  use_amp: true
  num_workers: 0

malfl:
  reward_epochs: 20000
  reward_lr: 0.001
  shaping_lr: 1.0e-4
  potential_epochs: 1000
  kl_penalty: 0.0
  prob_clip: 1.0e-6
  potential_reg_weight: 0.21
  policy_estimation_lr: 0.1
  policy_estimation_epochs: 10
  policy_entropy_coef: 0.3
  policy_estimation_batch: 8192
  reward_batch_size: 8192
  num_workers: 6

optimization:
  policy_lr: 0.01
  q_lr: 0.01
  value_regularizer: 0.0

logging:
  base_dir: outputs
  save_trajectories: false
  save_policies: true
  save_models: true
